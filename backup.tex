To enable both
of these two features, we develop
a novel theoretical framework.
On both synthetic data and existing services
we are providing, \eml can outperform 
baseline strategies by up to $XXX\times$.


\paragraph*{Summary of Technical Contributions}

\vspace{0.5em} \noindent
{\bf C1. (System Architecture and Problem Formulation)}
To our best knowledge, \eml is the first system of its kind
and our first contribution is the system design and the formulation
of the core technical problem to build \eml. 
\eml's design goal is two fold---(1) provide an abstraction
to enable more effective model exploration for our users, and 
(2) manage the shared infrastructure to enable more efficient
resource utilization of the exploration process 
for {\em all}, instead of, {\em one} of our users.

From user's perspective, \eml provides a simple high-level 
language to our user. In \eml, the user thinks about 
machine learning as a {\em arbitrary function approximator}.
To specify such an approximator, the users provides the
system (1) the size of the input, (2) the size of the output,
and (3) pairs of examples the function aims to approximate.
For example, if the user wants to train a classifier
for RGB images into 3 classes, she would submit a job like
\[
\texttt{Input = [256, 256, 3]
        Output = [3]}.
\]
\eml then automatically matches all {\em consistent} machine
learning models (e.g., AlexNet, ResNet-18, GoogLeNet, etc.) 
that can be used for users' application, and
explore all these models for users. Whenever a new model
finished running, \eml returns the model to the user.
Despite the simplicity of this abstraction, more than 70\% of
our users' applications can be build in this way.

From infrastructure's perspective, \eml then contains an
automatic scheduler to prioritize the execution of
different models for different users. The technical problem
here is {\em how can we come up with an execution strategy
(one execution order of all models) to maximize the
``global happiness'' of all of our users?} We formalize
this as the problem we call {\em multitenant model selection},
a novel technical problem that has not been studied before.

\vspace{0.5em} \noindent
{\bf C2. (Multitenant Multi-armed Bandits)} Our second contribution
is to provide the first algorithm for the multitenant model selection
problem. There are two challenges in applying existing approaches 
\begin{enumerate}
  \vspace{-0.25em}
  \item {\bf (Lack of Cost Model)} The first challenge in applying
  algorithms used in query optimizer is that it is difficult
  to come up with a {\em cost model} for a machine learning model
  and an application. It is difficult, if not impossible, to predict
  the quality of a machine learning model on a given data set without
  training (See Section~\ref{sec:related_work}: {\em Learning Curve Estimation}). 
  \vspace{-1.5em}
  \item {\bf (Multitenant)} The second challenge in applying
  algorithms used in automatic model selection, which does not 
  assume an explicit cost model is the existence of multiple users. 
  Because our objective changes to maximize the global happiness
  of all users instead of the individual happiness of a single user, 
  this changes both the algorithm and theory.
\end{enumerate}
\vspace{-0.25em}
In this paper, we develop a novel family of algorithms by extending 
classic \texttt{GP-UCB} algorithm for a singletenant model
selection to multitenant cases. We also prove the first 
known multitenant regret bound. Intuitively, our algorithm
acts in a hierarchical fashion --- first choose an user
to serve and then choose a model to explore for the chosen user.
For each user, our algorithm notes down the history of quality
improvements for the specific user, and making the decision
of {\em which user to serve next} by using these improvement histories. 
We develop multiple versions of our algorithm based on different
rules and analyze the theoretical properties for each.

{\bf (Cost Sensitive Singletenant Model Selection)} As different
machine learning model incurs different execution time, even
a singletenant model selection algorithm ought to be {\em cost-sensitive}.
One by-product of our study is the first {\em cost-sensitive
regret bound} for one singletenant algorithm. This is not part of
our central contributions, but is an interesting addition 
to model selection research.

\vspace{0.5em} \noindent
{\bf C3. (Evaluation)} Our third contribution is an intensive
evaluation of \eml on both synthetic data and two 
services we are providing to our users. We leave the 
details of the synthetic dataset to Section~\ref{sec:exp}
and describe the two services here.

{\bf (Image Classification with Deep Neural Networks)} When
the user submit jobs with the schema
\[
\texttt{Input = [A, B, 3]
        Output = [3]},
\]
\eml automatically matches the users' job with nine
different neural network architectures, including
NIN~\cite{XXX}, GoogLeNet~\cite{XXX}, ResNet-50~\cite{XXX}, AlexNet~\cite{XXX},
BN-AlexNet~\cite{XXX}, ResNet-18~\cite{XXX}, VGG-16~\cite{XXX}, VGG-19~\cite{XXX},
and SqueezeNet~\cite{XXX}. We collect twenty 
benchmark data sets and use each one to represent a single user.

{\bf (Binary Classification with Azure Machine Learning Studio)}
When the user submit jobs with the schema
\[
\texttt{Input = [A, B]
        Output = [1]},
\]
\eml automatically matches the users' job with eight
binary classification models available on 
Azure Machine Learning Studio~\cite{XXX},
including Averaged Perceptron~\cite{XXX}, Bayes Point Machine~\cite{XXX},
Boosted Decision Tree~\cite{XXX}, Decision Forests~\cite{XXX}, 
Decision Jungle~\cite{XXX}, Logistic Regression~\cite{XXX}, Neural Network~\cite{XXX},
and Support Vector Machine~\cite{XXX}. We collect seven
data sets from Kaggle and use each one to represent a single user.

Both services are what we are currently providing
to our users. We conduct experiments to compare
\eml's scheduler with classic schedulers such as
(1) Round Robin, (2) Random, and 
(3) User-Level Priority Queue. We show that,
our scheduler is able to outperform
these workload-agnostic schedulers
by up to XXX$\times$.

\paragraph*{Limitations} As the first attempt
of the multitenant model selection problem, 
we regard our algorithm not as a definitive answer
to this problem. Our current algorithm has
the following limitations. blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah.

\paragraph*{Overview} This paper is organized as follows.
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah
blah blah blah blah blah blah blah blah blah blah.


\section{Synthetic Data}\label{sec:appendix:synthetic}

We describe the algorithm we used to generate synthetic data 
for $N$ users and $M$ models. The basic assumption we have
is that the quality of a model $j$ for a user $i$ can be decomposed into
the following four factors:

\begin{enumerate}
\item User baseline quality $b_i$: Different users have different
      difficulties associated with their task. Some tasks have higher
      accuracy and some have lower accuracy. $b_i$ describes for each
      user how difficult the corresponding task is.
\item Model quality correlation $m_j$: For each user $i$, $\{m_j\}$
      corresponds to the fluctuation of the quality of model $j$ 
      over the baseline quality that can be explained by model correlation.
      For example, given a user dataset, when it is small, we should expect
      ResNet-20 always outperforms ResNet-50. This factor tries to capture
      correlations between models.
\item User quality correlation $u_i$: For each model $j$, $\{u_i\}$
      corresponds to the fluctuation of quality of model $j$ across different users
      over the baseline quality that can be explained by user correlation.
      For example, given a network such as ResNet-1000, we should expect
      all large datasets should all work quite well while all small datasets
      should all be worse. This factor tries to capture correlations between users.
\item White noise $\epsilon_{i,j}$: This factor tries to capture everything that are
      not explained by the previous three factors.
\end{enumerate}

Let $x_{i,j}$ be the quality of model $j$ for user $i$, the basic assumption we 
have to generate synthetic data is
\[
x_{i,j} = clip\_to\_0\_to\_1( b_i + m_j + u_i + \epsilon_{i,j} )
\]

The generative procedure for each $x_{i,j}$ is to draw samples 
$b_i$, $m_j$, $u_i$, $\epsilon_{i,j}$ from their corresponding 
distributions (defined as follows), and then sum them together.

\subsection{Generative Model}

The generative model behind the synthetic dataset we used is as follows.

\begin{enumerate}
\item {\bf Baseline Group:} There are different groups of tasks -- those that are difficult (with small
baseline quality) and those that are easy (with higher baseline quality). A given user can belong
to a single baseline group.
\item {\bf User Group:} A user group is a group of users who share the
same generative model of $u_i$. There are different groups -- high correlation groups
and low correlation groups. A given user can belong to a single user group.
\item {\bf Model Group:} A model group is a group of models who share
the same generative model of $m_j$. There are different groups -- high correlation groups
and low correlation groups. A given model can belong to a single model group.
\item {\bf White Noise:} We assume all white noises are i.i.d.
\end{enumerate}

Given a user who belongs to baseline group $B$ and user group $U$,
and a model that belongs to model group $M$, each of the above four terms
can be sampled from the corresponding distribution of the group.

\subsubsection{Generative Model of a Baseline Group}

Different baseline groups are governed by the expected quality $\mu_b$ of the group
and the variation of quality $\sigma_b$ within the group. Given a user
belong to a $(\mu_b, \sigma_b)$-baseline group, the user baseline quality 
$b$ is sampled as
\[
b \sim \mathcal{N}(\mu_b, \sigma_b).
\]

\subsubsection{Generative Model of a Model Group}

Different model groups are governed by a constant $\sigma_M$ corresponding to
the strength of correlation. Let ${M_1,...,M_K}$ be all models in a
$\sigma_M$-model group, we assume the following generative models.

\paragraph*{Hidden Similarity between Models}

One assumption we have is that each model has a similarity measure with each
other that is {\em not known to the algorithm}. This similarity defines how strong
the correlations are between different models. For each 
model $M_j$, we assign 
\[
f(M_j) \sim \mathcal{\mathcal{U}}(0, 1)
\]
and will use it later to calculate the similarity between models.

\paragraph*{Sample from a model group}

Given a $\sigma_M$-model group and the $f(-)$ score already pre-assigned to each
model, we sample 
\[
[m_1,...m_K] \sim \mathcal{N}(0, \Sigma_M)
\]
where the covariance matrix $\Sigma_M$ is defined as follows.

For each pair of $(i,j)$, $\Sigma_M[i,j]$ follows the following intuition:
If $f(M_i)$ and $f(M_j)$ are close to each other, then their correlation
is stronger. Thus, we define
\[
\Sigma_M[i,j] = \exp \left\{-\frac{(f(M_i) - f(M_j))^2}{\sigma_M^2} \right\}
\]

\subsubsection{Generative Model of the User Group}

Different user groups are governed by a constant $\sigma_U$ corresponding to
the strength of correlation. Let ${M_1,...,M_K}$ be all users in a
$\lambda_U$-user group, the generative model is similar to that of a model group.
\[
[u_1,...,u_K] \sim \mathcal{N}(0, \Sigma_U).
\]

\subsubsection{Generative Model of the White Noise}

All white noises are i.i.d, governed by the same constant $\sigma_W$.
\[
\epsilon_{i,j} \sim \mathcal{N}(0, \sigma_W).
\]


\subsection{Dataset Generation}

A synthetic dataset is governed by the following tuple
\begin{eqnarray*}
( 
\mathcal{B} = \{(\mu_b^{(i)}, \sigma_b^{(i)})\},
\mathcal{M} = \{(\sigma_M^{(j)})\},
\mathcal{U} = \{(\sigma_U^{(i)})\},
\sigma_W,  \\
p_U: \mathcal{B} \times \mathcal{U} \mapsto \mathbb{N},
p_M: \mathcal{M} \mapsto \mathbb{N}
)
\end{eqnarray*}
which is generated by 
$|\mathcal{B}|$ baseline groups, 
$|\mathcal{M}|$ model groups, 
and $|\mathcal{U}|$ user groups. $p_U$
maps each combination of baseline group
and user group to the number of users
belonging to the given combination.
$p_M$ maps each model group to how many
models belong to the given group.

\subsubsection{Instantiation 1}

The first synthetic dataset uses the following instantiation.

\begin{enumerate}
\item $\mathcal{B} = \{(0.75, \sigma_{B}), (0.25, \sigma_{B})\}$
\item $\mathcal{M} = \{\sigma_{M}\}$
\item $\mathcal{U} = \{\sigma_U\}$
\item $p_U(*)$ = 50
\item $p_M(*)$ = 100
\end{enumerate}
which will generate 100 models and 100 users. We first
choose $\sigma_{B} = 1$, $\sigma_{M} = 1$, and $\sigma_{U} = 1$.

To understand the impact of different factors to the algorithm,
\begin{enumerate}
\item Change $\sigma_{M}$ to understand the impact of
correlations between models.
\item Change $\sigma_{U}$ to understand the impact of
correlations between users.
\item Change $P_U$ to understand the impact of
mixing users with different task difficulties (if all users' task are
equally hard, why not round-robin)
\end{enumerate}


%Notes: \\

%training and testing (train on 90 users and test on 10 users) \\

%metrics: average and worst-case accuracy loss (averaged over all users) \\

%Explain why accuracy loss not regret and why accuracy loss is bounded by regret \\

%Model-irrelevant noise means user baseline quality (so stronger model-irrelevant noise means reducing the impact of model correlation, i.e., a smaller $\alpha$).\\

%Impact of cost-awareness: when reach the same accuracy loss, how much total cost have we spent?

\vspace{-1em}
\paragraph*{Limitations}
As the first attempt
of the multitenant model selection problem,
this paper has the following limitations.
From the theortical perspective, the regret bound
is not yet theortically optimal and tight.
From the practical perspective,
the current framework only supports the case where
the whole GPU pool are treated as a single device.
With our current scale (29 GPUs), this is not
a problem as we can achieve linear scale up
with all GPUs to train a single model. However, as our service grows
rapidly, we need to extend our current framework
to be aware of multiple devices in the near future.
Another limitation is that our
analysis focuses on GP-UCB and it is not clear
how to integrate other algorithms such as
GP-EI and GP-PI into a multi-tenant 
framework. Last, we define the global
happiness of all users as the sum 
of their regrets. It is not clear how to
integrate hard rules such as ``deadline''
associated with each user and design
algorithms for other aggregation functions.