@ARTICLE{bowman:reasoning,
	AUTHOR = "Mic Bowman and Saumya K. Debray and Larry L. Peterson",
	TITLE = "Reasoning About Naming Systems",
	JOURNAL = "ACM Trans. Program. Lang. Syst.",
	VOLUME = {15},
	NUMBER = {5},
	PAGES = {795-825},
	MONTH = "November",
	YEAR = {1993}	}

@ARTICLE{braams:babel,
	AUTHOR = "Johannes Braams",
	TITLE = "Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles",
	JOURNAL = {TUGboat},
	VOLUME = {12},
	NUMBER = {2},
	PAGES = {291-301},
	MONTH = "June",
	YEAR = {1991}	}

@INPROCEEDINGS{clark:pct,
	AUTHOR = "Malcolm Clark",
	TITLE = "Post Congress Tristesse",
	BOOKTITLE = "TeX90 Conference Proceedings",
	PAGES = "84-89",
	ORGANIZATION = "TeX Users Group",
	MONTH = "March", 
	YEAR = {1991}	}

@ARTICLE{herlihy:methodology,
	AUTHOR = "Maurice Herlihy",
	TITLE = "A Methodology for Implementing Highly Concurrent
	Data Objects",
	JOURNAL = {ACM Trans. Program. Lang. Syst.},
	VOLUME = {15},
	NUMBER = {5},
	PAGES = {745-770},
	MONTH = "November",
	YEAR = {1993}	}

@BOOK{Lamport:LaTeX,
	AUTHOR = "Leslie Lamport",
	TITLE = "LaTeX User's Guide and Document Reference Manual",
	PUBLISHER = "Addison-Wesley Publishing Company",
	ADDRESS = "Reading, Massachusetts",
	YEAR = "1986"	}

@BOOK{salas:calculus,
	AUTHOR = "S.L. Salas and Einar Hille",
	TITLE = "Calculus: One and Several Variable",
	PUBLISHER = "John Wiley and Sons",
	ADDRESS = "New York",
	YEAR = "1978"	}


@article{Wae41,
  author="van der Waerden, Bartel L.",
  title="Die lange {R}eichweite der regelm{\"a}{\ss}igen {A}tomanordnung in {M}ischkristallen",
  journal="Zeitschrift f{\"u}r Physik",
  volume="118",
  number="7",
  pages="473--488",
  year={1941},
  issn="0044-3328",
  doi="10.1007/BF01342928",
  url="http://dx.doi.org/10.1007/BF01342928"
}

@misc{Peres17,
  author = {Yuval Peres},
  year = {2017},
  howpublished = {personal communication}
}

@incollection {torpid,
    AUTHOR = {Borgs, Christian and Chayes, Jennifer T. and Frieze, Alan and
              Kim, Jeong Han and Tetali, Prasad and Vigoda, Eric and Vu, Van
              Ha},
     TITLE = {Torpid mixing of some {M}onte {C}arlo {M}arkov chain
              algorithms in statistical physics},
 BOOKTITLE = {FOCS},
     PAGES = {218--229},
      YEAR = {1999},
   MRCLASS = {68Q25 (82B80)},
  MRNUMBER = {1917562},
       DOI = {10.1109/SFFCS.1999.814594},
       URL = {http://dx.doi.org/10.1109/SFFCS.1999.814594},
}

@article{CGW16,
  author    = {Jin{-}Yi Cai and
               Heng Guo and
               Tyson Williams},
  title     = {A Complete Dichotomy Rises from the Capture of Vanishing Signatures},
  journal   = {{SIAM} J. Comput.},
  volume    = {45},
  number    = {5},
  pages     = {1671--1728},
  year      = {2016},
  url       = {http://dx.doi.org/10.1137/15M1049798},
  doi       = {10.1137/15M1049798},
}

@article{DGHMT16,
  author    = {Hugo Duminil-Copin and Maxime Gagnebin and Matan Harel and Ioan Manolescu and Vincent Tassion},
  title     = {Discontinuity of the phase transition for the planar random-cluster and {P}otts models with $q>4$},
  journal   = {ArXiv,},
  volume    = {\href{http://arxiv.org/abs/1611.09877}{abs/1611.09877}},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09877},
}

@article{GL16,
  author    = {Reza Gheissari and Eyal Lubetzky},
  title     = {Mixing times of critical {2D} {P}otts models},
  journal   = {ArXiv,},
  volume    = {\href{http://arxiv.org/abs/1607.02182}{abs/1607.02182}},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.02182},
}

@ARTICLE{Collevecchio:worm,
   author = {Andrea Collevecchio and Timothy M. Garoni and Timothy Hyndman and Daniel Tokarev},
    title = {The worm process for the {I}sing model is rapidly mixing},
  journal = {J. Stat. Phys.},
     year = {2016},
   volume = {164},
   number = {5},
    pages = {1082--1102},
      doi = {10.1007/s10955-016-1572-2},
}

@article {DS,
    AUTHOR = {Diaconis, Persi and Stroock, Daniel},
     TITLE = {Geometric bounds for eigenvalues of {M}arkov chains},
   JOURNAL = {Ann. Appl. Probab.},
  FJOURNAL = {The Annals of Applied Probability},
    VOLUME = {1},
      YEAR = {1991},
    NUMBER = {1},
     PAGES = {36--61},
      ISSN = {1050-5164},
   MRCLASS = {60J10},
  MRNUMBER = {1097463},
MRREVIEWER = {Gregory F. Lawler},
       URL = {http://links.jstor.org/sici?sici=1050-5164(199102)1:1<36:GBFEOM>2.0.CO;2-E&origin=MSN},
}

@article{Ullrich:Z2,
  author    = {Mario Ullrich},
  title     = {Comparison of {S}wendsen-{W}ang and heat-bath dynamics},
  journal   = {Random Struct. Algorithms},
  volume    = {42},
  number    = {4},
  pages     = {520--535},
  year      = {2013},
  doi       = {http://dx.doi.org/10.1002/rsa.20431},
}

@article{Ullrich:Comparison,
  author    = {Mario Ullrich},
  title     = {Swendsen-{W}ang Is Faster than Single-Bond Dynamics},
  journal   = {{SIAM} J. Discrete Math.},
  volume    = {28},
  number    = {1},
  pages     = {37--48},
  year      = {2014},
  url       = {http://dx.doi.org/10.1137/120864003},
  doi       = {10.1137/120864003},
  timestamp = {Thu, 24 Apr 2014 10:47:16 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/siamdm/Ullrich14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Ullrich:dissertation,
  author    = {Mario Ullrich},
  title     = {Rapid mixing of {S}wendsen-{W}ang dynamics in two dimensions},
  journal   = {Dissertationes Mathematicae}, 
  volume    = {502},
  year      = {2014},
}

@article {Tutte2,
    AUTHOR = {Goldberg, Leslie Ann and Jerrum, Mark},
     TITLE = {The complexity of computing the sign of the {T}utte
              polynomial},
   JOURNAL = {SIAM J. Comput.},
  FJOURNAL = {SIAM Journal on Computing},
    VOLUME = {43},
      YEAR = {2014},
    NUMBER = {6},
     PAGES = {1921--1952},
      ISSN = {0097-5397},
   MRCLASS = {68Q17 (05C30 05C31 68Q25 68R10)},
  MRNUMBER = {3291539},
       DOI = {10.1137/12088330X},
       URL = {http://dx.doi.org/10.1137/12088330X},
}
		
@article {Tutte1,
    AUTHOR = {Goldberg, Leslie Ann and Jerrum, Mark},
     TITLE = {Inapproximability of the {T}utte polynomial},
   JOURNAL = {Inform. and Comput.},
  FJOURNAL = {Information and Computation},
    VOLUME = {206},
      YEAR = {2008},
    NUMBER = {7},
     PAGES = {908--929},
      ISSN = {0890-5401},
   MRCLASS = {05B35},
  MRNUMBER = {2433765},
       DOI = {10.1016/j.ic.2008.04.003},
       URL = {http://dx.doi.org/10.1016/j.ic.2008.04.003},
}
		

@article{JSperm,
  author    = {Mark Jerrum and
               Alistair Sinclair},
  title     = {Approximating the Permanent},
  journal   = {{SIAM} J. Comput.},
  volume    = {18},
  number    = {6},
  pages     = {1149--1178},
  year      = {1989},
  url       = {http://dx.doi.org/10.1137/0218077},
  doi       = {10.1137/0218077},
  timestamp = {Mon, 12 Sep 2011 16:10:07 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/siamcomp/JerrumS89},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@book{LPW06,
AUTHOR = {Levin, David A. and Peres, Yuval and Wilmer, Elizabeth L.},
TITLE = {Markov chains and mixing times},
PUBLISHER = {American Mathematical Society},
ADDRESS = {Providence, RI},
YEAR = {2009},
PAGES = {xviii+371},
ISBN = {978-0-8218-4739-8},
}

@book{Jerrum03,
        author = {Jerrum, Mark},
        title  = {Counting, Sampling and Integrating: Algorithms and Complexity},
        Publisher = {Birkh\"auser},
        Year   = {2003},
        Series = {Lectures in Mathematics, ETH Z\"urich}
}

@article {SJ,
    AUTHOR = {Sinclair, Alistair and Jerrum, Mark},
     TITLE = {Approximate counting, uniform generation and rapidly mixing
              {M}arkov chains},
   JOURNAL = {Inform. and Comput.},
  FJOURNAL = {Information and Computation},
    VOLUME = {82},
      YEAR = {1989},
    NUMBER = {1},
     PAGES = {93--133},
      ISSN = {0890-5401},
   MRCLASS = {68Q25 (60J10 68R05 68R10)},
  MRNUMBER = {1003059},
       DOI = {10.1016/0890-5401(89)90067-9},
       URL = {http://dx.doi.org/10.1016/0890-5401(89)90067-9},
}

@article{DS93,
  author = "Diaconis, Persi and Saloff-Coste, Laurent",
  doi = "10.1214/aoap/1177005359",
  fjournal = "The Annals of Applied Probability",
  journal = "Ann. Appl. Probab.",
  month = "08",
  number = "3",
  pages = "696--730",
  publisher = "The Institute of Mathematical Statistics",
  title = "Comparison Theorems for Reversible {M}arkov Chains",
  url = "http://dx.doi.org/10.1214/aoap/1177005359",
  volume = "3",
  year = "1993"
}

@article {comparison,
    AUTHOR = {Dyer, Martin and Goldberg, Leslie Ann and Jerrum, Mark and
              Martin, Russell},
     TITLE = {Markov chain comparison},
   JOURNAL = {Probab. Surv.},
  FJOURNAL = {Probability Surveys},
    VOLUME = {3},
      YEAR = {2006},
     PAGES = {89--111},
      ISSN = {1549-5787},
   MRCLASS = {60J10 (60J27 68W20)},
  MRNUMBER = {2216963},
MRREVIEWER = {Martin V. Hildebrand},
       DOI = {10.1214/154957806000000041},
       URL = {http://dx.doi.org/10.1214/154957806000000041},
}

@article{EdwardsSokal,
    AUTHOR = {Edwards, Robert G. and Sokal, Alan D.},
     TITLE = {Generalization of the {F}ortuin-{K}asteleyn-{S}wendsen-{W}ang
              representation and {M}onte {C}arlo algorithm},
   JOURNAL = {Phys. Rev. D (3)},
  FJOURNAL = {Physical Review. D. Particles and Fields. Third Series},
    VOLUME = {38},
      YEAR = {1988},
    NUMBER = {6},
     PAGES = {2009--2012},
      ISSN = {0556-2821},
     CODEN = {PRVDAQ},
   MRCLASS = {82-04 (81-08 81E25 82-08 82A68)},
  MRNUMBER = {965465},
       DOI = {10.1103/PhysRevD.38.2009},
       URL = {http://dx.doi.org/10.1103/PhysRevD.38.2009},
}

@Article{LS12,
  author="Eyal Lubetzky and Allan Sly",
  title="Critical {I}sing on the Square Lattice Mixes in Polynomial Time",
  journal="Commun. Math. Phys.",
  year="2012",
  volume="313",
  number="3",
  pages="815--836",
}

@article {FortuinKasteleyn,
    AUTHOR = {Fortuin, C. M. and Kasteleyn, P. W.},
     TITLE = {On the random-cluster model. {I}. {I}ntroduction and relation
              to other models},
   JOURNAL = {Physica},
    VOLUME = {57},
      YEAR = {1972},
     PAGES = {536--564},
   MRCLASS = {82.60},
  MRNUMBER = {0359655},
MRREVIEWER = {S. V. Temko},
}

@article{Sin92,
  author    = {Alistair Sinclair},
  title     = {Improved Bounds for Mixing Rates of {M}arkov Chains and Multicommodity
               Flow},
  journal   = {Comb. Probab. Comp.},
  volume    = {1},
  pages     = {351--370},
  year      = {1992},
  url       = {http://dx.doi.org/10.1017/S0963548300000390},
  doi       = {10.1017/S0963548300000390},
  timestamp = {Wed, 05 Jan 2011 09:34:11 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/cpc/Sinclair92},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{RW99,
  author    = {Dana Randall and
               David Wilson},
  title     = {Sampling Spin Configurations of an {I}sing System},
  booktitle = {SODA},
  pages     = {959--960},
  year      = {1999},
  crossref  = {DBLP:conf/soda/1999},
  url       = {http://dl.acm.org/citation.cfm?id=314500.314945},
  timestamp = {Mon, 14 Mar 2016 15:38:52 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/soda/RandallW99},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{PSworm,
  title = {Worm Algorithms for Classical Statistical Models},
  author = {Prokof'ev, Nikolay and Svistunov, Boris},
  journal = {Phys. Rev. Lett.},
  volume = {87},
  issue = {16},
  pages = {160601},
  numpages = {4},
  year = {2001},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.87.160601},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.87.160601}
}

@article {GoreJerrum,
    AUTHOR = {Gore, Vivek K. and Jerrum, Mark R.},
     TITLE = {The {S}wendsen-{W}ang process does not always mix rapidly},
   JOURNAL = {J. Stat. Phys.},
  FJOURNAL = {Journal of Statistical Physics},
    VOLUME = {97},
      YEAR = {1999},
    NUMBER = {1-2},
     PAGES = {67--86},
      ISSN = {0022-4715},
     CODEN = {JSTPSB},
   MRCLASS = {82C20 (05C80 82B20 82B80 82C27 82C80)},
  MRNUMBER = {1733467},
MRREVIEWER = {Alan D. Sokal},
       DOI = {10.1023/A:1004610900745},
       URL = {http://dx.doi.org/10.1023/A:1004610900745},
}

@article {GrimmettJanson,
    AUTHOR = {Grimmett, Geoffrey and Janson, Svante},
     TITLE = {Random even graphs},
   JOURNAL = {Electron. J. Combin.},
  FJOURNAL = {Electronic Journal of Combinatorics},
    VOLUME = {16},
      YEAR = {2009},
    NUMBER = {1},
     PAGES = {Research Paper, 46, 19},
      ISSN = {1077-8926},
   MRCLASS = {05C80 (60K35)},
  MRNUMBER = {2491648},
MRREVIEWER = {Dirk Oliver Theis},
       URL = {http://www.combinatorics.org/Volume_16/Abstracts/v16i1r46.html},
}

@article {JSising,
    AUTHOR = {Jerrum, Mark and Sinclair, Alistair},
     TITLE = {Polynomial-time approximation algorithms for the {I}sing
              model},
   JOURNAL = {SIAM J. Comput.},
  FJOURNAL = {SIAM Journal on Computing},
    VOLUME = {22},
      YEAR = {1993},
    NUMBER = {5},
     PAGES = {1087--1116},
      ISSN = {0097-5397},
     CODEN = {SMJCAT},
   MRCLASS = {82B20 (60J10 60K35 82B80)},
  MRNUMBER = {1237164},
       DOI = {10.1137/0222066},
       URL = {http://dx.doi.org/10.1137/0222066},
}

@article {BollobasGrimmettJanson,
    AUTHOR = {Bollob{\'a}s, Bel{\'a} and Grimmett, Geoffrey and Janson, Svante},
     TITLE = {The random-cluster model on the complete graph},
   JOURNAL = {Probab. Theory Related Fields},
  FJOURNAL = {Probability Theory and Related Fields},
    VOLUME = {104},
      YEAR = {1996},
    NUMBER = {3},
     PAGES = {283--317},
      ISSN = {0178-8051},
     CODEN = {PTRFEU},
   MRCLASS = {05C80 (60K35 82B20)},
  MRNUMBER = {1376340},
MRREVIEWER = {Wei-Shih Yang},
       DOI = {10.1007/BF01213683},
       URL = {http://dx.doi.org/10.1007/BF01213683},
}

@article {GoldbergJerrum:Potts,
    AUTHOR = {Goldberg, Leslie Ann and Jerrum, Mark},
     TITLE = {Approximating the partition function of the ferromagnetic
              {P}otts model},
   JOURNAL = {J. ACM},
  FJOURNAL = {Journal of the ACM},
    VOLUME = {59},
      YEAR = {2012},
    NUMBER = {5},
     PAGES = {Art. 25, 31},
      ISSN = {0004-5411},
   MRCLASS = {82B20 (05C31 05C65 68Q17)},
  MRNUMBER = {2995824},
       DOI = {10.1145/2371656.2371660},
       URL = {http://dx.doi.org/10.1145/2371656.2371660},
}

@InProceedings{BlancaSinclair:Meanfield,
  author =	{Antonio Blanca and Alistair Sinclair},
  title =	{{Dynamics for the Mean-field Random-cluster Model}},
  booktitle = {RANDOM},
  pages =	{528--543},
  publisher =	{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},  
  year =	{2015},
  doi =		{http://dx.doi.org/10.4230/LIPIcs.APPROX-RANDOM.2015.528},
}

@inproceedings{GalanisSV:SwendsenWang,
  author    = {Andreas Galanis and Daniel {\v{S}}tefankovi\v{c} and Eric Vigoda},
  title     = {Swendsen-{W}ang Algorithm on the Mean-Field {P}otts Model},
  booktitle = {RANDOM},
  pages     = {815--828},
  year      = {2015},
  crossref  = {DBLP:conf/approx/2015},
  doi       = {http://dx.doi.org/10.4230/LIPIcs.APPROX-RANDOM.2015.815},
}

@article{LongNNY:SwendsenWang,
  author    = {Yun Long and Asaf Nachmias and Weiyang Ning and Yuval Peres},
  title     = {A Power Law of Order $1/4$ for Critical Mean Field {S}wendsen-{W}ang Dynamics},
  journal   = {Memoirs of the AMS},
  volume    = {232},
  number    = {1092},
  year      = {2014},
}

@Article{BlancaSinclair:Z2,
  author="Blanca, Antonio and Sinclair, Alistair",
  title="Random-cluster dynamics in $\mathbb{Z}^2$",
  journal="Probab.\ Theory Related Fields",
  year="2016",
  note = {DOI: \href{https://doi.org/10.1007/s00440-016-0725-1}{10.1007/s00440-016-0725-1}},
}


@book{Grimmett:book,
  title={The random-cluster model},
  author={Grimmett, Geoffrey},
  year={2006},
  publisher={Springer}
}

@article{SwendsenWang,
  title = {Nonuniversal critical dynamics in {M}onte {C}arlo simulations},
  author = {Swendsen, Robert and Wang, Jian-Sheng},
  journal = {Phys. Rev. Lett.},
  volume = {58},
  issue = {2},
  pages = {86--88},
  numpages = {0},
  year = {1987},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.58.86},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.58.86}
}

@article {PW13,
AUTHOR = {Peres, Yuval and Winkler, Peter},
TITLE = {Can extra updates delay mixing?},
JOURNAL = {Comm. Math. Phys.},
VOLUME = {323},
YEAR = {2013},
NUMBER = {3},
PAGES = {1007--1016},
}

@Article{Holroyd11,
author={Holroyd, Alexander E.},
title={Some Circumstances Where Extra Updates Can Delay Mixing},
journal={J. Stat. Phys.},
year="2011",
volume="145",
number="6",
pages="1649--1652",
}

@inproceedings{HDMR16,
  author    = {Bryan D. He and
               Christopher {De Sa} and
               Ioannis Mitliagkas and
               Christopher R{\'{e}}},
  title     = {Scan Order in {G}ibbs Sampling: Models in Which it Matters and Bounds
               on How Much},
  booktitle = {NIPS},
  pages     = {1--9},
  year      = {2016},
}

@article{RR15,
	author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
	title = {Surprising Convergence Properties of Some Simple {G}ibbs Samplers under Various Scans},
	journal = {Int. J. Stat. Probab.},
	volume = {5},
	number = {1},
	year = {2015},
}

@article{DGJ08,
  author    = {Martin E. Dyer and
               Leslie Ann Goldberg and
               Mark Jerrum},
  title     = {Dobrushin Conditions and Systematic Scan},
  journal   = {Combin. Probab. Comput.},
  volume    = {17},
  number    = {6},
  pages     = {761--779},
  year      = {2008},
}

@article{DGJ06,
  author    = {Martin E. Dyer and
               Leslie Ann Goldberg and
               Mark Jerrum},
  title     = {Systematic scan for sampling colourings},
  journal   = {Ann. Appl. Probab.},
  volume    = {16},
  number    = {1},
  pages     = {185-230},
  year      = {2006},
}

@inproceedings{BD97,
  author    = {Russ Bubley and
               Martin E. Dyer},
  title     = {Path Coupling: {A} Technique for Proving Rapid Mixing in {M}arkov Chains},
  booktitle = {FOCS},
  pages     = {223--231},
  year      = {1997},
}

@inproceedings{Tosh16,
  author    = {Christopher Tosh},
  title     = {Mixing Rates for the Alternating {G}ibbs Sampler over Restricted {B}oltzmann
               Machines and Friends},
  booktitle = {ICML},
  pages     = {840--849},
  year      = {2016},
}

@inproceedings{RR12,
  author    = {Benjamin Recht and
               Christopher R{\'{e}}},
  title     = {Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures,
               Case-studies, and Consequences},
  booktitle = {COLT},
  pages     = {11.1--11.24},
  year      = {2012},
}

@article{GOP17,
        author  = {Mert G\"urb\"uzbalaban, Asu Ozdaglar, and Pablo Parrilo},
        title   = {Convergence rate of incremental aggregated gradient algorithms},
        journal = {SIAM J.~Optimiz.},
        year    = {2017},
        note    = {\textit{To appear}},
}

@article{DR00,
author = "Diaconis, Persi and Ram, Arun",
journal = "Michigan Math. J.",
number = "1",
pages = "157--190",
publisher = "University of Michigan, Department of Mathematics",
title = "Analysis of systematic scan {M}etropolis algorithms using {I}wahori-{H}ecke algebra techniques.",
url = "http://dx.doi.org/10.1307/mmj/1030132713",
volume = "48",
year = "2000"
}

@inproceedings{Hayes06,
  author    = {Thomas P. Hayes},
  title     = {A simple condition implying rapid mixing of single-site dynamics on spin systems},
  booktitle = {FOCS},
  pages     = {39--46},
  year      = {2006},
}

@article{Dobrushin70,
        author = {Dobrushin, R. L.},
        title  = {Prescribing a system of random variables by conditional distributions},
        year   = {1970},
        journal = {Theory Probab. Appl.},
        issue  = {15},
        number = {3},
        pages  = {458--486},
}

@article{Fill91,
        author = {James A. Fill},
        title  = {Eigenvalue bounds on convergence to stationary for nonreversible {M}arkov chains, with an application to the exclusion process},
        year   = {1991},
        journal = {Ann. Appl. Probab.},
        volume  = {1},
        number = {1},
        pages  = {62-87},
}

@article{MS13,
  author    = {Elchanan Mossel and Allan Sly},
  title     = {Exact thresholds for {I}sing-{G}ibbs samplers on general graphs},
  journal   = {Ann. Probab.},
  year      = {2013},
  volume    = {41},
  number    = {1},
  pages     = {294-328},
}

@article{LS88,
 author = {Gregory F. Lawler and Alan D. Sokal},
 journal = {Trans. Amer. Math. Soc.},
 number = {2},
 pages = {557-580},
 publisher = {American Mathematical Society},
 title = {Bounds on the $L^2$ Spectrum for {M}arkov Chains and Markov Processes: A Generalization of {C}heeger's Inequality},
 volume = {309},
 year = {1988}
}

@inproceedings{HKS15,
  author    = {Daniel J. Hsu and
               Aryeh Kontorovich and
               Csaba Szepesv{\'{a}}ri},
  title     = {Mixing Time Estimation in Reversible {M}arkov Chains from a Single Sample Path},
  booktitle = {NIPS},
  pages     = {1459--1467},
  year      = {2015},
}

@inproceedings{BBM11,
  author    = {Nayantara Bhatnagar and
               Andrej Bogdanov and
               Elchanan Mossel},
  title     = {The Computational Complexity of Estimating {MCMC} Convergence Time},
  booktitle = {RANDOM},
  pages     = {424--435},
  year      = {2011},
}

%%%%%

@InProceedings{Salakhutdinov:2009:AISTATS,
  title =    {Deep Boltzmann Machines},
  author =   {Ruslan Salakhutdinov and Geoffrey Hinton},
  booktitle =    {AISTATS},
  pages =    {448--455},
  year =   {2009},
}

@article{Blei:2003:JMLR,
 author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
 title = {Latent Dirichlet Allocation},
 journal = {J. Mach. Learn. Res.},
 year = {2003},
 pages = {993--1022},
} 


@article{Smolensky:1986:IPD,
 author = {Smolensky, P.},
 journal = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
 year = {1986},
 pages = {194--281},
} 

@book{Li:2009:Book,
 author = {Li, Stan Z.},
 title = {Markov Random Field Modeling in Image Analysis},
 year = {2009},
} 

@inproceedings{Lafferty:2001:ICML,
  author    = {John D. Lafferty and Andrew McCallum and Fernando C. N. Pereira  },
  title     = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
  booktitle = {ICML},
  pages     = {282--289},
  year      = {2001},
}


@article{Luo16,
  author    = {Gang Luo},
  title     = {A review of automatic selection methods for machine learning algorithms
               and hyper-parameter values},
  journal   = {NetMAHIB},
  volume    = {5},
  number    = {1},
  pages     = {18},
  year      = {2016},
}

@inproceedings{SparksTHFJK15,
  author    = {Evan R. Sparks and
               Ameet Talwalkar and
               Daniel Haas and
               Michael J. Franklin and
               Michael I. Jordan and
               Tim Kraska},
  title     = {Automating model search for large scale machine learning},
  booktitle = {{SoCC}},
  pages     = {368--380},
  year      = {2015}
}

@Article{Robbins:1952,
  author =       "Robbins, Herbert",
  title =        "Some aspects of the sequential design of experiments",
  journal =      "Bulletin of the American Mathematical Society",
  year =         "1952",
  volume =    "58",
  number =    "5",
  pages =     "527--535"
}

@article{Lai:1985,
 author = {Lai, T.L and Robbins, Herbert},
 title = {Asymptotically Efficient Adaptive Allocation Rules},
 journal = {Adv. Appl. Math.},
 volume = {6},
 number = {1},
 year = {1985},
 pages = {4--22}
}

@inbook{Agrawal1995,
    author = {Agrawal, Rajeev},
    booktitle = {Advances in Applied Probability},
    pages = {1054--1078},
    publisher = {Applied Probability Trust},
    title = {{Sample mean based index policies with O(log n) regret for the multi-armed bandit problem.}},
    volume = {27},
    year = {1995}
}

@article{Auer02,
  author    = {Peter Auer},
  title     = {Using Confidence Bounds for Exploitation-Exploration Trade-offs},
  journal   = {Journal of Machine Learning Research},
  volume    = {3},
  pages     = {397--422},
  year      = {2002}
}

@article{Auer:2002,
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
 title = {Finite-time Analysis of the Multiarmed Bandit Problem},
 journal = {Mach. Learn.},
 volume = {47},
 number = {2-3},
 year = {2002}
}

@inproceedings{SrinivasKKS10,
  author    = {Niranjan Srinivas and
               Andreas Krause and
               Sham Kakade and
               Matthias W. Seeger},
  title     = {Gaussian Process Optimization in the Bandit Setting: No Regret and
               Experimental Design},
  booktitle = {ICML},
  pages     = {1015--1022},
  year      = {2010}
}

@article{Kushner1964,
    author = {Kushner, H. J.},
    journal = {Journal of Basic Engineering},
    number = {1},
    title = {{A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise}},
    volume = {86},
    year = {1964}
}

@inproceedings{SnoekLA12,
  author    = {Jasper Snoek and
               Hugo Larochelle and
               Ryan P. Adams},
  title     = {Practical Bayesian Optimization of Machine Learning Algorithms},
  pages     = {2960--2968},
  year      = {2012}
}

@inproceedings{ChapelleL11,
  author    = {Olivier Chapelle and Lihong Li},
  title     = {An Empirical Evaluation of Thompson Sampling},
  booktitle = {{NIPS}},
  pages     = {2249--2257},
  year      = {2011}
}

@inproceedings{KaufmannKM12,
  author    = {Emilie Kaufmann and
               Nathaniel Korda and
               R{\'{e}}mi Munos},
  title     = {Thompson Sampling: An Asymptotically Optimal Finite-Time Analysis},
  booktitle = {{ALT}},
  pages     = {199--213},
  year      = {2012}
}

@article{Scott:2010,
 author = {Scott, Steven L.},
 title = {A Modern Bayesian Look at the Multi-armed Bandit},
 journal = {Appl. Stoch. Model. Bus. Ind.},
 volume = {26},
 number = {6},
 year = {2010},
 pages = {639--658}
} 

@misc{Lichman:2013,
author = "M. Lichman",
year = "2013",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences"
}

@article{DelgadoCBA14,
  author    = {Manuel Fern{\'{a}}ndez Delgado and
               Eva Cernadas and
               Sen{\'{e}}n Barro and
               Dinani Gomes Amorim},
  title     = {Do we need hundreds of classifiers to solve real world classification
               problems?},
  journal   = {Journal of Machine Learning Research},
  volume    = {15},
  number    = {1},
  pages     = {3133--3181},
  year      = {2014}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WHOLE MENDELEY LIB FROM CZ %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




@inproceedings{DavidShue2012,
author = {{David Shue} and {Michael J. Freedman} and {Anees Shaikh}},
booktitle = {OSDI},
keywords = {multi-tenant},
mendeley-tags = {multi-tenant},
title = {{Performance Isolation and Fairness for Multi-Tenant Cloud Storage | USENIX}},
url = {https://www.usenix.org/node/170865},
year = {2012}
}
@inproceedings{Krebs2014,
author = {Krebs, Rouven and Spinner, Simon and Ahmed, Nadia and Kounev, Samuel},
booktitle = {2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
doi = {10.1109/CCGrid.2014.80},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Krebs et al. - 2014 - Resource Usage Control in Multi-tenant Applications.pdf:pdf},
isbn = {978-1-4799-2784-5},
keywords = {multitenant},
mendeley-tags = {multitenant},
month = {may},
pages = {122--131},
publisher = {IEEE},
title = {{Resource Usage Control in Multi-tenant Applications}},
url = {http://ieeexplore.ieee.org/document/6846447/},
year = {2014}
}
@inproceedings{JonathanMace2015,
author = {{Jonathan Mace} and {Peter Bodik} and {Rodrigo Fonseca} and {Madanlal Musuvathi}},
booktitle = {NSDI},
keywords = {multitenant},
mendeley-tags = {multitenant},
title = {{Retro: Targeted Resource Management in Multi-tenant Distributed Systems | USENIX}},
url = {https://www.usenix.org/node/188953},
year = {2015}
}
@article{DBLP:journals/ftml/BubeckC12,
author = {Bubeck, S{\'{e}}bastien and Cesa-Bianchi, Nicol{\`{o}}},
doi = {10.1561/2200000024},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--122},
title = {{Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems}},
url = {https://doi.org/10.1561/2200000024},
volume = {5},
year = {2012}
}
@inproceedings{Zhang2017a,
address = {New York, New York, USA},
author = {Zhang, Ce and Wu, Wentao and Li, Tian},
booktitle = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics  - HILDA'17},
doi = {10.1145/3077257.3077265},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Wu, Li - 2017 - An Overreaction to the Broken Machine Learning Abstraction The ease.ml Vision.pdf:pdf},
isbn = {9781450350297},
pages = {1--6},
publisher = {ACM Press},
title = {{An Overreaction to the Broken Machine Learning Abstraction: The ease.ml Vision}},
url = {http://dl.acm.org/citation.cfm?doid=3077257.3077265},
year = {2017}
}
@misc{gpyopt2016,
author = {GPyOpt},
howpublished = {$\backslash$url{\{}http://github.com/SheffieldML/GPyOpt{\}}},
title = {{{\{}GPyOpt{\}}: A Bayesian Optimization framework in python}},
year = {2016}
}
@inproceedings{Tamagnini2017,
address = {New York, New York, USA},
author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
booktitle = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics  - HILDA'17},
doi = {10.1145/3077257.3077260},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Tamagnini et al. - 2017 - Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations.pdf:pdf},
isbn = {9781450350297},
keywords = {classification,explanation,machine learning,visual analytics},
pages = {1--6},
publisher = {ACM Press},
title = {{Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations}},
url = {http://dl.acm.org/citation.cfm?doid=3077257.3077260},
year = {2017}
}
@inproceedings{Varma2017,
address = {New York, New York, USA},
author = {Varma, Paroma and Iter, Dan and {De Sa}, Christopher and R{\'{e}}, Christopher},
booktitle = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics  - HILDA'17},
doi = {10.1145/3077257.3077263},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Varma et al. - 2017 - Flipper A Systematic Approach to Debugging Training Sets.pdf:pdf},
isbn = {9781450350297},
pages = {1--5},
publisher = {ACM Press},
title = {{Flipper: A Systematic Approach to Debugging Training Sets}},
url = {http://dl.acm.org/citation.cfm?doid=3077257.3077263},
year = {2017}
}
@inproceedings{Krishnan2017,
address = {New York, New York, USA},
author = {Krishnan, Sanjay and Wu, Eugene},
booktitle = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics  - HILDA'17},
doi = {10.1145/3077257.3077271},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Krishnan, Wu - 2017 - PALM Machine Learning Explanations For Iterative Debugging.pdf:pdf},
isbn = {9781450350297},
pages = {1--6},
publisher = {ACM Press},
title = {{PALM: Machine Learning Explanations For Iterative Debugging}},
url = {http://dl.acm.org/citation.cfm?doid=3077257.3077271},
year = {2017}
}
@book{Binnig2016,
abstract = {Held June 26, 2016, in San Francisco, California, in conjunction with SIGMOD/PODS 2016.},
author = {Binnig, Carsten and Fekete, Alan and Nandi, Arnab and {Association for Computing Machinery} and {ACM-Sigmod International Conference on Management of Data (2016 : San Francisco}, Calif.) and {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (2016 : San Francisco}, Calif.)},
isbn = {9781450342070},
pages = {93},
publisher = {ACM},
title = {{Proceedings of the Workshop on Human-In-the-Loop Data Analytics}},
url = {http://dl.acm.org/citation.cfm?id=2939502},
year = {2016}
}
@article{Bailis2017,
abstract = {Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.},
archivePrefix = {arXiv},
arxivId = {1705.07538},
author = {Bailis, Peter and Olukotun, Kunle and Re, Christopher and Zaharia, Matei},
eprint = {1705.07538},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Bailis et al. - 2017 - Infrastructure for Usable Machine Learning The Stanford DAWN Project.pdf:pdf},
journal = {arXiv },
month = {may},
title = {{Infrastructure for Usable Machine Learning: The Stanford DAWN Project}},
url = {http://arxiv.org/abs/1705.07538},
year = {2017}
}
@inproceedings{AbadiMBarhamPChenJChenZDavisADeanJDevinMGhemawatSIrvingGIsardMKudlurMLevenbergJMongaRMooreSMurrayDGSteinerBTuckerPVasudevanVWardenPWickeMYuY2016,
author = {M, Abadi and P, Barham and J, Chen and Z, Chen and A, Davis and J, Dean and M, Devin and S, Ghemawat and G, Irving and M, Isard and M, Kudlur and J, Levenberg and R, Monga and S, Moore and DG, Murray and B, Steiner and P, Tucker and V, Vasudevan and P, Warden and M, Wicke and Y, Yu and X, Zheng},
booktitle = {OSDI},
title = {{TensorFlow: A System for Large-Scale Machine Learning | USENIX}},
url = {https://www.usenix.org/node/199317},
year = {2016}
}
@inproceedings{Alexandrov2015,
address = {New York, New York, USA},
author = {Alexandrov, Alexander and Kunft, Andreas and Katsifodimos, Asterios and Sch{\"{u}}ler, Felix and Thamsen, Lauritz and Kao, Odej and Herb, Tobias and Markl, Volker},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data - SIGMOD '15},
doi = {10.1145/2723372.2750543},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Alexandrov et al. - 2015 - Implicit Parallelism through Deep Language Embedding.pdf:pdf},
isbn = {9781450327589},
keywords = {control flow,data-parallel execution,large-scale data analysis,mapreduce,monad comprehensions,scala macros},
pages = {47--61},
publisher = {ACM Press},
title = {{Implicit Parallelism through Deep Language Embedding}},
url = {http://dl.acm.org/citation.cfm?doid=2723372.2750543},
year = {2015}
}
@book{Meng2001,
abstract = {Vol. 5 onward called also Print-archive edition and represents articles already published online. Issues for 2005- published by Microtome publishing.},
author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
booktitle = {The Journal of Machine Learning Research},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Meng et al. - 2001 - MLlib machine learning in apache spark.pdf:pdf},
number = {1},
pages = {1235--1241},
publisher = {MIT Press},
title = {{MLlib: machine learning in apache spark}},
url = {http://dl.acm.org/citation.cfm?id=2946679},
volume = {17},
year = {2016}
}
@article{Boehm2016a,
author = {Boehm, Matthias and Surve, Arvind C. and Tatikonda, Shirish and Dusenberry, Michael W. and Eriksson, Deron and Evfimievski, Alexandre V. and Manshadi, Faraz Makari and Pansare, Niketan and Reinwald, Berthold and Reiss, Frederick R. and Sen, Prithviraj},
doi = {10.14778/3007263.3007279},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Boehm et al. - 2016 - SystemML Declarative Machine Learning on Spark.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {mlsys},
mendeley-tags = {mlsys},
month = {sep},
number = {13},
pages = {1425--1436},
publisher = {VLDB Endowment},
title = {{SystemML: Declarative Machine Learning on Spark}},
url = {http://dl.acm.org/citation.cfm?doid=3007263.3007279},
volume = {9},
year = {2016}
}
@article{Narasayya2015,
author = {Narasayya, Vivek and Menache, Ishai and Singh, Mohit and Li, Feng and Syamala, Manoj and Chaudhuri, Surajit},
doi = {10.14778/2752939.2752942},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Narasayya et al. - 2015 - Sharing buffer pool memory in multi-tenant relational database-as-a-service.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {multitenant},
mendeley-tags = {multitenant},
month = {feb},
number = {7},
pages = {726--737},
publisher = {VLDB Endowment},
title = {{Sharing buffer pool memory in multi-tenant relational database-as-a-service}},
url = {http://dl.acm.org/citation.cfm?doid=2752939.2752942},
volume = {8},
year = {2015}
}
@article{Tan2016,
author = {Tan, Zilong and Babu, Shivnath},
doi = {10.14778/2977797.2977799},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Babu - 2016 - Tempo robust and self-tuning resource management in multi-tenant parallel databases.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {multitenant},
mendeley-tags = {multitenant},
month = {jun},
number = {10},
pages = {720--731},
publisher = {VLDB Endowment},
title = {{Tempo: robust and self-tuning resource management in multi-tenant parallel databases}},
url = {http://dl.acm.org/citation.cfm?doid=2977797.2977799},
volume = {9},
year = {2016}
}
@article{Bellare2013,
author = {Bellare, Kedar and Curino, Carlo and Machanavajihala, Ashwin and Mika, Peter and Rahurkar, Mandar and Sane, Aamod},
doi = {10.14778/2536222.2536236},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Bellare et al. - 2013 - WOO a scalable and multi-tenant platform for continuous knowledge base synthesis.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {multitenant},
mendeley-tags = {multitenant},
month = {aug},
number = {11},
pages = {1114--1125},
publisher = {VLDB Endowment},
title = {{WOO: a scalable and multi-tenant platform for continuous knowledge base synthesis}},
url = {http://dl.acm.org/citation.cfm?doid=2536222.2536236},
volume = {6},
year = {2013}
}
@article{Das2013,
author = {Das, Sudipto and Narasayya, Vivek R. and Li, Feng and Syamala, Manoj},
doi = {10.14778/2732219.2732223},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Das et al. - 2013 - CPU sharing techniques for performance isolation in multi-tenant relational database-as-a-service.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {multitenant},
mendeley-tags = {multitenant},
month = {sep},
number = {1},
pages = {37--48},
publisher = {VLDB Endowment},
title = {{CPU sharing techniques for performance isolation in multi-tenant relational database-as-a-service}},
url = {http://dl.acm.org/citation.cfm?doid=2732219.2732223},
volume = {7},
year = {2013}
}
@article{Desautels2014,
author = {Desautels, Thomas and Krause, Andreas and Burdick, Joel W.},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Desautels, Krause, Burdick - 2014 - Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {4053--4103},
title = {{Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization}},
url = {http://jmlr.org/papers/v15/desautels14a.html},
volume = {15},
year = {2014}
}
@book{Goovaerts1997,
abstract = {1. Introduction -- 2. Exploratory data analysis -- 3. The random function model -- 4. Inference and modeling -- 5. Local estimation: Accounting for a single attribute -- 6. Local estimation: Accounting for secondary information -- 7. Assessment of local uncertainty -- 8. Assessment of spatial uncertainty -- 9. Summary -- A. Fitting an LMC -- B. List of acronyms and notation -- C. The Jura data.},
author = {Goovaerts, Pierre.},
isbn = {9780195115383},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {483},
publisher = {Oxford University Press},
title = {{Geostatistics for natural resources evaluation}},
url = {https://global.oup.com/academic/product/geostatistics-for-natural-resources-evaluation-9780195115383?cc=ch{\&}lang=en{\&}},
year = {1997}
}
@incollection{Hutter2011,
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {LION},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {978-3-642-25565-6},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {507--523},
publisher = {Springer-Verlag},
title = {{Sequential Model-Based Optimization for General Algorithm Configuration}},
url = {http://link.springer.com/10.1007/978-3-642-25566-3{\_}40},
year = {2011}
}
@inproceedings{Bardenet2013,
author = {Bardenet, R{\'{e}}mi and Brendel, M{\'{a}}ty{\'{a}}s and K{\'{e}}gl, Bal{\'{a}}zs and Sebag, Mich{\`{e}}le},
booktitle = {ICML},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {II--199},
publisher = {JMLR.org},
title = {{Collaborative hyperparameter tuning}},
url = {http://dl.acm.org/citation.cfm?id=3042916},
year = {2013}
}
@inproceedings{Swersky2013,
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
booktitle = {NIPS},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Swersky, Snoek, Adams - 2013 - Multi-Task Bayesian Optimization.pdf:pdf},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {2004--2012},
title = {{Multi-Task Bayesian Optimization}},
url = {https://papers.nips.cc/paper/5086-multi-task-bayesian-optimization},
year = {2013}
}
@inproceedings{Feurer2015,
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
booktitle = {NIPS},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Feurer et al. - 2015 - Efficient and Robust Automated Machine Learning.pdf:pdf},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {2962--2970},
title = {{Efficient and Robust Automated Machine Learning}},
url = {https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning},
year = {2015}
}
@inproceedings{Snoek2012,
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {NIPS},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - 2012 - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {2951--2959},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms},
year = {2012}
}
@inproceedings{Sparks2015,
address = {New York, New York, USA},
author = {Sparks, Evan R. and Talwalkar, Ameet and Haas, Daniel and Franklin, Michael J. and Jordan, Michael I. and Kraska, Tim},
booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing - SoCC '15},
doi = {10.1145/2806777.2806945},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Sparks et al. - 2015 - Automating model search for large scale machine learning.pdf:pdf},
isbn = {9781450336512},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {368--380},
publisher = {ACM Press},
title = {{Automating model search for large scale machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2806777.2806945},
year = {2015}
}
@inproceedings{Golovin2017,
author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John Elliot and Sculley, D.},
booktitle = {KDD},
keywords = {model selection},
mendeley-tags = {model selection},
title = {{Google Vizier: A Service for Black-Box Optimization}},
url = {https://research.google.com/pubs/pub46180.html},
year = {2017}
}
@article{Kotthoff2017,
author = {Kotthoff, Lars and Thornton, Chris and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Kotthoff et al. - 2017 - Auto-WEKA 2.0 Automatic model selection and hyperparameter optimization in WEKA.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {model selection},
mendeley-tags = {model selection},
number = {25},
pages = {1--5},
title = {{Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA}},
url = {http://www.jmlr.org/papers/v18/16-261.html},
volume = {18},
year = {2017}
}
@inproceedings{Thornton2013,
address = {New York, New York, USA},
author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
doi = {10.1145/2487575.2487629},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Thornton et al. - 2013 - Auto-WEKA.pdf:pdf},
isbn = {9781450321747},
keywords = {model selection},
mendeley-tags = {model selection},
pages = {847},
publisher = {ACM Press},
title = {{Auto-WEKA}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487629},
year = {2013}
}
@article{Salimans2017,
abstract = {We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using hundreds to thousands of parallel workers, ES can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
archivePrefix = {arXiv},
arxivId = {1703.03864},
author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
eprint = {1703.03864},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:pdf},
journal = {arXiv},
keywords = {DFO},
mendeley-tags = {DFO},
month = {mar},
title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.03864},
year = {2017}
}
@article{Czarnecki2017,
abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
archivePrefix = {arXiv},
arxivId = {1703.00522},
author = {Czarnecki, Wojciech Marian and {\'{S}}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
eprint = {1703.00522},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Czarnecki et al. - 2017 - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf:pdf},
journal = {arXiv},
keywords = {DFO},
mendeley-tags = {DFO},
month = {mar},
title = {{Understanding Synthetic Gradients and Decoupled Neural Interfaces}},
url = {http://arxiv.org/abs/1703.00522},
year = {2017}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
eprint = {1410.0759},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Chetlur et al. - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {oct},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@book{Forum1994,
author = {Forum and P, Message},
keywords = {mpi},
mendeley-tags = {mpi},
publisher = {University of Tennessee},
title = {{MPI: A Message-Passing Interface Standard}},
url = {http://dl.acm.org/citation.cfm?id=898758},
year = {1994}
}
@article{Russakovsky2015,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {data},
mendeley-tags = {data},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://link.springer.com/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@inproceedings{Awan2016,
address = {New York, New York, USA},
author = {Awan, A. A. and Hamidouche, K. and Venkatesh, A. and Panda, D. K.},
booktitle = {Proceedings of the 23rd European MPI Users' Group Meeting on   - EuroMPI 2016},
doi = {10.1145/2966884.2966912},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Awan et al. - 2016 - Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep Learning.pdf:pdf},
isbn = {9781450342346},
keywords = {mpi},
mendeley-tags = {mpi},
pages = {15--22},
publisher = {ACM Press},
title = {{Efficient Large Message Broadcast using NCCL and CUDA-Aware MPI for Deep Learning}},
url = {http://dl.acm.org/citation.cfm?doid=2966884.2966912},
year = {2016}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Computation},
keywords = {networks},
mendeley-tags = {networks},
month = {nov},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {ArXiv},
keywords = {networks},
mendeley-tags = {networks},
month = {feb},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
journal = {ICLR},
keywords = {networks},
mendeley-tags = {networks},
title = {{VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION}},
url = {https://arxiv.org/pdf/1409.1556.pdf},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
journal = {ArXiv},
keywords = {networks},
mendeley-tags = {networks},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
journal = {NIPS},
keywords = {networks},
mendeley-tags = {networks},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Astrahan1976,
author = {Astrahan, M. M. and Mehl, J. W. and Putzolu, G. R. and Traiger, I. L. and Wade, B. W. and Watson, V. and Blasgen, M. W. and Chamberlin, D. D. and Eswaran, K. P. and Gray, J. N. and Griffiths, P. P. and King, W. F. and Lorie, R. A. and McJones, P. R.},
doi = {10.1145/320455.320457},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Astrahan et al. - 1976 - System R relational approach to database management.pdf:pdf},
journal = {ACM Transactions on Database Systems},
keywords = {db},
mendeley-tags = {db},
month = {jun},
number = {2},
pages = {97--137},
publisher = {ACM},
title = {{System R: relational approach to database management}},
url = {http://portal.acm.org/citation.cfm?doid=320455.320457},
volume = {1},
year = {1976}
}
@inproceedings{TrishulChilimbi2014,
author = {{Trishul Chilimbi} and {Yutaka Suzue} and {Johnson Apacible} and {Karthik Kalyanaraman}},
booktitle = {OSDI},
keywords = {system},
mendeley-tags = {system},
title = {{Project Adam: building an efficient and scalable deep learning training system}},
url = {http://dl.acm.org/citation.cfm?id=2685094},
year = {2014}
}
@misc{Dean2012,
author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
keywords = {system},
mendeley-tags = {system},
pages = {1223--1231},
publisher = {Curran Associates Inc.},
title = {{Large scale distributed deep networks}},
url = {http://dl.acm.org/citation.cfm?id=2999271},
year = {2012}
}
@article{Moritz2015,
abstract = {Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1511.06051},
author = {Moritz, Philipp and Nishihara, Robert and Stoica, Ion and Jordan, Michael I.},
eprint = {1511.06051},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Moritz et al. - 2015 - SparkNet Training Deep Networks in Spark.pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {nov},
title = {{SparkNet: Training Deep Networks in Spark}},
url = {http://arxiv.org/abs/1511.06051},
year = {2015}
}
@article{Iandola2015,
abstract = {Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs.},
archivePrefix = {arXiv},
arxivId = {1511.00175},
author = {Iandola, Forrest N. and Ashraf, Khalid and Moskewicz, Matthew W. and Keutzer, Kurt},
eprint = {1511.00175},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Iandola et al. - 2015 - FireCaffe near-linear acceleration of deep neural network training on compute clusters.pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {oct},
title = {{FireCaffe: near-linear acceleration of deep neural network training on compute clusters}},
url = {http://arxiv.org/abs/1511.00175},
year = {2015}
}
@article{Chen2015a,
abstract = {MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
archivePrefix = {arXiv},
arxivId = {1512.01274},
author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
eprint = {1512.01274},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2015 - MXNet A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems(3).pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {dec},
title = {{MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1512.01274},
year = {2015}
}
@article{Wang2016,
abstract = {Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multi-modal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by non-experts without much effort especially when the model is large and complex. The other is scalability, that is the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on GPUs as well as on CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.},
archivePrefix = {arXiv},
arxivId = {1603.07846},
author = {Wang, Wei and Chen, Gang and Chen, Haibo and Dinh, Tien Tuan Anh and Gao, Jinyang and Ooi, Beng Chin and Tan, Kian-Lee and Wang, Sheng},
eprint = {1603.07846},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - Deep Learning At Scale and At Ease.pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {mar},
title = {{Deep Learning At Scale and At Ease}},
url = {http://arxiv.org/abs/1603.07846},
year = {2016}
}
@article{Hadjis2016,
abstract = {We study the factors affecting training time in multi-device deep learning systems. Given a specification of a convolutional neural network, our goal is to minimize the time to train this model on a cluster of commodity CPUs and GPUs. We first focus on the single-node setting and show that by using standard batching and data-parallel techniques, throughput can be improved by at least 5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training speed directly proportional to the throughput of a device regardless of its underlying hardware, allowing each node in the cluster to be treated as a black box. Our second contribution is a theoretical and empirical study of the tradeoffs affecting end-to-end training time in a multiple-device setting. We identify the degree of asynchronous parallelization as a key factor affecting both hardware and statistical efficiency. We see that asynchrony can be viewed as introducing a momentum term. Our results imply that tuning momentum is critical in asynchronous parallel configurations, and suggest that published results that have not been fully tuned might report suboptimal performance for some configurations. For our third contribution, we use our novel understanding of the interaction between system and optimization dynamics to provide an efficient hyperparameter optimizer. Our optimizer involves a predictive model for the total time to convergence and selects an allocation of resources to minimize that time. We demonstrate that the most popular distributed deep learning systems fall within our tradeoff space, but do not optimize within the space. By doing this optimization, our prototype runs 1.9x to 12x faster than the fastest state-of-the-art systems.},
archivePrefix = {arXiv},
arxivId = {1606.04487},
author = {Hadjis, Stefan and Zhang, Ce and Mitliagkas, Ioannis and Iter, Dan and R{\'{e}}, Christopher},
eprint = {1606.04487},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Hadjis et al. - 2016 - Omnivore An Optimizer for Multi-device Deep Learning on CPUs and GPUs.pdf:pdf},
journal = {ArXiv},
keywords = {us},
mendeley-tags = {us},
month = {jun},
title = {{Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs}},
url = {http://arxiv.org/abs/1606.04487},
year = {2016}
}
@inproceedings{gupta2015deep,
author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
booktitle = {ICML},
keywords = {precision},
mendeley-tags = {precision},
pages = {1737--1746},
title = {{Deep Learning with Limited Numerical Precision.}},
year = {2015}
}
@article{hubara2016quantized,
author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
journal = {arXiv preprint arXiv:1609.07061},
keywords = {precision},
mendeley-tags = {precision},
title = {{Quantized neural networks: Training neural networks with low precision weights and activations}},
year = {2016}
}
@article{zhang2016zipml,
author = {Zhang, Hantian and Kara, Kaan and Li, Jerry and Alistarh, Dan and Liu, Ji and Zhang, Ce},
journal = {arXiv preprint arXiv:1611.05402},
keywords = {precision},
mendeley-tags = {precision},
title = {{ZipML: An End-to-end Bitwise Framework for Dense Generalized Linear Models}},
year = {2016}
}
@article{gong2014compressing,
author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
journal = {arXiv preprint arXiv:1412.6115},
keywords = {precision},
mendeley-tags = {precision},
title = {{Compressing deep convolutional networks using vector quantization}},
year = {2014}
}
@inproceedings{vanhoucke2011improving,
author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mark Z},
booktitle = {Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop},
keywords = {precision},
mendeley-tags = {precision},
organization = {Citeseer},
pages = {4},
title = {{Improving the speed of neural networks on CPUs}},
volume = {1},
year = {2011}
}
@inproceedings{gopi2013one,
author = {Gopi, Sivakant and Netrapalli, Praneeth and Jain, Prateek and Nori, Aditya V},
booktitle = {ICML (3)},
keywords = {precision},
mendeley-tags = {precision},
pages = {154--162},
title = {{One-Bit Compressed Sensing: Provable Support and Vector Recovery.}},
year = {2013}
}
@article{lesser2011effects,
author = {Lesser, Bernd and M{\"{u}}cke, Manfred and Gansterer, Wilfried N},
journal = {Procedia Computer Science},
keywords = {precision},
mendeley-tags = {precision},
pages = {508--517},
publisher = {Elsevier},
title = {{Effects of reduced precision on floating-point SVM classification accuracy}},
volume = {4},
year = {2011}
}
@inproceedings{seide20141,
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
booktitle = {Interspeech},
keywords = {precision},
mendeley-tags = {precision},
pages = {1058--1062},
title = {{1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.}},
year = {2014}
}
@inproceedings{de2015taming,
author = {{De Sa}, Christopher M and Zhang, Ce and Olukotun, Kunle and R{\'{e}}, Christopher},
booktitle = {Advances in neural information processing systems},
keywords = {precision},
mendeley-tags = {precision},
pages = {2674--2682},
title = {{Taming the wild: A unified analysis of hogwild-style algorithms}},
year = {2015}
}
@inproceedings{rastegari2016xnor,
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
booktitle = {European Conference on Computer Vision},
keywords = {precision},
mendeley-tags = {precision},
organization = {Springer},
pages = {525--542},
title = {{Xnor-net: Imagenet classification using binary convolutional neural networks}},
year = {2016}
}
@article{miyashita2016convolutional,
author = {Miyashita, Daisuke and Lee, Edward H and Murmann, Boris},
journal = {arXiv preprint arXiv:1603.01025},
keywords = {precision},
mendeley-tags = {precision},
title = {{Convolutional neural networks using logarithmic data representation}},
year = {2016}
}
@inproceedings{cortes2010impact,
author = {Cortes, Corinna and Mohri, Mehryar and Talwalkar, Ameet},
booktitle = {AISTATS},
keywords = {precision},
mendeley-tags = {precision},
pages = {113--120},
title = {{On the Impact of Kernel Approximation on Learning Accuracy.}},
year = {2010}
}
@article{alistarh2016qsgd,
author = {Alistarh, Dan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
journal = {arXiv preprint arXiv:1610.02132},
keywords = {precision},
mendeley-tags = {precision},
title = {{QSGD: Randomized Quantization for Communication-Optimal Stochastic Gradient Descent}},
year = {2016}
}
@article{lin2015fixed,
author = {Lin, Darryl D and Talathi, Sachin S and Annapureddy, V Sreekanth},
journal = {arXiv, page},
keywords = {precision},
mendeley-tags = {precision},
title = {{Fixed point quantization of deep convolutional networks}},
year = {2015}
}
@article{zhou2016dorefa,
author = {Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
journal = {arXiv preprint arXiv:1606.06160},
keywords = {precision},
mendeley-tags = {precision},
title = {{DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients}},
year = {2016}
}
@article{Han2016,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
eprint = {1510.00149},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Han, Mao, Dally - 2016 - Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.pdf:pdf},
journal = {ICLR},
keywords = {precision},
mendeley-tags = {precision},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {http://arxiv.org/abs/1510.00149},
year = {2016}
}
@article{Xie2015,
abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1511.06335},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
eprint = {1511.06335},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Girshick, Farhadi - 2015 - Unsupervised Deep Embedding for Clustering Analysis.pdf:pdf},
journal = {ArXiv},
keywords = {gan},
mendeley-tags = {gan},
month = {nov},
title = {{Unsupervised Deep Embedding for Clustering Analysis}},
url = {http://arxiv.org/abs/1511.06335},
year = {2015}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {ICLR},
keywords = {optimization},
mendeley-tags = {optimization},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Yu2016,
author = {Yu, Kun-Hsing and Zhang, Ce and Berry, Gerald J. and Altman, Russ B. and R{\'{e}}, Christopher and Rubin, Daniel L. and Snyder, Michael},
doi = {10.1038/ncomms12474},
journal = {Nature Communications},
keywords = {us},
mendeley-tags = {us},
month = {aug},
pages = {12474},
publisher = {Nature Publishing Group},
title = {{Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features}},
url = {http://www.nature.com/doifinder/10.1038/ncomms12474},
volume = {7},
year = {2016}
}
@article{2002PASP..114.1051S,
author = {Starck, J.{\~{}}L. and Pantin, E and Murtagh, F},
doi = {10.1086/342606},
journal = {$\backslash$pasp},
keywords = {Methods: Data Analysis,Techniques: Image Processing,astro},
mendeley-tags = {astro},
month = {oct},
pages = {1051--1069},
title = {{Deconvolution in Astronomy: A Review}},
volume = {114},
year = {2002}
}
@article{shannon49,
author = {Shannon, C E},
doi = {10.1109/JRPROC.1949.232969},
issn = {0096-8390},
journal = {Proceedings of the IRE},
keywords = {Bandwidth;Circuits;Communication systems;Electron,astro},
mendeley-tags = {astro},
month = {jan},
number = {1},
pages = {10--21},
title = {{Communication in the Presence of Noise}},
volume = {37},
year = {1949}
}
@article{2007A&A...461..373M,
author = {Magain, P and Courbin, F and Gillon, M and Sohy, S and Letawe, G and Chantry, V and Letawe, Y},
doi = {10.1051/0004-6361:20042505},
journal = {$\backslash$aap},
keywords = {astro,techniques: image processing,techniques: photometric},
mendeley-tags = {astro},
month = {jan},
pages = {373--379},
title = {{A deconvolution-based algorithm for crowded field photometry with unknown point spread function}},
volume = {461},
year = {2007}
}
@article{lucy1974iterative,
author = {Lucy, Leon B},
journal = {The astronomical journal},
keywords = {astro},
mendeley-tags = {astro},
pages = {745},
title = {{An iterative technique for the rectification of observed distributions}},
volume = {79},
year = {1974}
}
@article{2000AJ....120.1579Y,
author = {York, D.{\~{}}G. and Adelman, J and {Anderson Jr.}, J.{\~{}}E. and Anderson, S.{\~{}}F. and Annis, J and Bahcall, N.{\~{}}A. and Bakken, J.{\~{}}A. and Barkhouser, R and Bastian, S and Berman, E and Boroski, W.{\~{}}N. and Bracker, S and Briegel, C and Briggs, J.{\~{}}W. and Brinkmann, J and Brunner, R and Burles, S and Carey, L and Carr, M.{\~{}}A. and Castander, F.{\~{}}J. and Chen, B and Colestock, P.{\~{}}L. and Connolly, A.{\~{}}J. and Crocker, J.{\~{}}H. and Csabai, I and Czarapata, P.{\~{}}C. and Davis, J.{\~{}}E. and Doi, M and Dombeck, T and Eisenstein, D and Ellman, N and Elms, B.{\~{}}R. and Evans, M.{\~{}}L. and Fan, X and Federwitz, G.{\~{}}R. and Fiscelli, L and Friedman, S and Frieman, J.{\~{}}A. and Fukugita, M and Gillespie, B and Gunn, J.{\~{}}E. and Gurbani, V.{\~{}}K. and de Haas, E and Haldeman, M and Harris, F.{\~{}}H. and Hayes, J and Heckman, T.{\~{}}M. and Hennessy, G.{\~{}}S. and Hindsley, R.{\~{}}B. and Holm, S and Holmgren, D.{\~{}}J. and Huang, C.-h. and Hull, C and Husby, D and Ichikawa, S.-I. and Ichikawa, T and Ivezi{\'{c}}, {\v{Z}} and Kent, S and Kim, R.{\~{}}S.{\~{}}J. and Kinney, E and Klaene, M and Kleinman, A.{\~{}}N. and Kleinman, S and Knapp, G.{\~{}}R. and Korienek, J and Kron, R.{\~{}}G. and Kunszt, P.{\~{}}Z. and Lamb, D.{\~{}}Q. and Lee, B and Leger, R.{\~{}}F. and Limmongkol, S and Lindenmeyer, C and Long, D.{\~{}}C. and Loomis, C and Loveday, J and Lucinio, R and Lupton, R.{\~{}}H. and MacKinnon, B and Mannery, E.{\~{}}J. and Mantsch, P.{\~{}}M. and Margon, B and McGehee, P and McKay, T.{\~{}}A. and Meiksin, A and Merelli, A and Monet, D.{\~{}}G. and Munn, J.{\~{}}A. and Narayanan, V.{\~{}}K. and Nash, T and Neilsen, E and Neswold, R and Newberg, H.{\~{}}J. and Nichol, R.{\~{}}C. and Nicinski, T and Nonino, M and Okada, N and Okamura, S and Ostriker, J.{\~{}}P. and Owen, R and Pauls, A.{\~{}}G. and Peoples, J and Peterson, R.{\~{}}L. and Petravick, D and Pier, J.{\~{}}R. and Pope, A and Pordes, R and Prosapio, A and Rechenmacher, R and Quinn, T.{\~{}}R. and Richards, G.{\~{}}T. and Richmond, M.{\~{}}W. and Rivetta, C.{\~{}}H. and Rockosi, C.{\~{}}M. and Ruthmansdorfer, K and Sandford, D and Schlegel, D.{\~{}}J. and Schneider, D.{\~{}}P. and Sekiguchi, M and Sergey, G and Shimasaku, K and Siegmund, W.{\~{}}A. and Smee, S and Smith, J.{\~{}}A. and Snedden, S and Stone, R and Stoughton, C and Strauss, M.{\~{}}A. and Stubbs, C and SubbaRao, M and Szalay, A.{\~{}}S. and Szapudi, I and Szokoly, G.{\~{}}P. and Thakar, A.{\~{}}R. and Tremonti, C and Tucker, D.{\~{}}L. and Uomoto, A and {Vanden Berk}, D and Vogeley, M.{\~{}}S. and Waddell, P and Wang, S.-i. and Watanabe, M and Weinberg, D.{\~{}}H. and Yanny, B and Yasuda, N and {SDSS Collaboration}},
doi = {10.1086/301513},
journal = {$\backslash$aj},
keywords = {Cosmology: Observations,Instrumentation: Miscellaneous,astro},
mendeley-tags = {astro},
month = {sep},
pages = {1579--1587},
title = {{The Sloan Digital Sky Survey: Technical Summary}},
volume = {120},
year = {2000}
}
@article{Reed:2016:ArXiv,
archivePrefix = {arXiv},
arxivId = {1605.05396},
author = {Reed, S and Akata, Z and Yan, X and Logeswaran, L and Schiele, B and Lee, H},
eprint = {1605.05396},
journal = {ArXiv e-prints},
keywords = {Computer Science - Computer Vision and Pattern Re,Computer Science - Neural and Evolutionary Computi,astro},
mendeley-tags = {astro},
month = {may},
title = {{Generative Adversarial Text to Image Synthesis}},
year = {2016}
}
@article{2015ApJS..219...12A,
archivePrefix = {arXiv},
arxivId = {astro-ph.IM/1501.00963},
author = {Alam, S and Albareti, F.{\~{}}D. and {Allende Prieto}, C and Anders, F and Anderson, S.{\~{}}F. and Anderton, T and Andrews, B.{\~{}}H. and Armengaud, E and Aubourg, {\'{E}} and Bailey, S and Et al.},
doi = {10.1088/0067-0049/219/1/12},
eprint = {1501.00963},
journal = {$\backslash$apjs},
keywords = {astro,atlases,catalogs,surveys},
mendeley-tags = {astro},
month = {jul},
pages = {12},
primaryClass = {astro-ph.IM},
title = {{The Eleventh and Twelfth Data Releases of the Sloan Digital Sky Survey: Final Data from SDSS-III}},
volume = {219},
year = {2015}
}
@article{Sola:1997:NuclearScience,
author = {Sola, J and Sevilla, J},
doi = {10.1109/23.589532},
issn = {0018-9499},
journal = {IEEE Transactions on Nuclear Science},
keywords = {astro},
mendeley-tags = {astro},
month = {jun},
number = {3},
pages = {1464--1468},
title = {{Importance of input data normalization for the application of neural networks to complex industrial problems}},
volume = {44},
year = {1997}
}
@article{1996FCPh...17...95B,
author = {Buta, R and Combes, F},
journal = {$\backslash$fcp},
keywords = {BARS,DYNAMICS,EVOLUTION,GALAXIES: STRUCTURE,RINGS,SIMULATIONS,astro},
mendeley-tags = {astro},
pages = {95--281},
title = {{Galactic Rings}},
volume = {17},
year = {1996}
}
@article{1998ApJ...494..472M,
author = {Magain, P and Courbin, F and Sohy, S},
doi = {10.1086/305187},
journal = {$\backslash$apj},
keywords = {METHODS: DATA ANALYSIS,METHODS: NUMERICAL,Methods: Data Analysis,Methods: Numerical,TECHNIQUES: IMAGE PROCESSING,Techniques: Image Processing,astro},
mendeley-tags = {astro},
month = {feb},
pages = {472--477},
title = {{Deconvolution with Correct Sampling}},
volume = {494},
year = {1998}
}
@article{richardson1972bayesian,
author = {Richardson, William Hadley},
journal = {JOSA},
keywords = {astro},
mendeley-tags = {astro},
number = {1},
pages = {55--59},
publisher = {Optical Society of America},
title = {{Bayesian-based iterative method of image restoration}},
volume = {62},
year = {1972}
}
@article{bell1995information,
author = {Bell, Anthony J and Sejnowski, Terrence J},
journal = {Neural computation},
keywords = {astro},
mendeley-tags = {astro},
number = {6},
pages = {1129--1159},
publisher = {MIT Press},
title = {{An information-maximization approach to blind separation and blind deconvolution}},
volume = {7},
year = {1995}
}
@article{Goodfellow:2014:NIPS,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
journal = {NIPS},
keywords = {astro},
mendeley-tags = {astro},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
year = {2014}
}
@article{2007MNRAS.378...83L,
author = {Letawe, G and Magain, P and Courbin, F and Jablonka, P and Jahnke, K and Meylan, G and Wisotzki, L},
doi = {10.1111/j.1365-2966.2007.11741.x},
journal = {$\backslash$mnras},
keywords = {astro,galaxies: active,galaxies: interactions,galaxies: stellar content,quasars: general,techniques: image processing,techniques: spectroscopic},
mendeley-tags = {astro},
month = {jun},
pages = {83--108},
title = {{On-axis spectroscopy of the host galaxies of 20 optically luminous quasars at z {\~{}} 0.3}},
volume = {378},
year = {2007}
}
@article{2009arXiv0912.0201L,
archivePrefix = {arXiv},
arxivId = {astro-ph.IM/0912.0201},
author = {{LSST Science Collaboration} and Abell, P.{\~{}}A. and Allison, J and Anderson, S.{\~{}}F. and Andrew, J.{\~{}}R. and Angel, J.{\~{}}R.{\~{}}P. and Armus, L and Arnett, D and Asztalos, S.{\~{}}J. and Axelrod, T.{\~{}}S. and Et al.},
eprint = {0912.0201},
journal = {ArXiv e-prints},
keywords = {Astrophysics - Cosmology and Extragalactic Astrop,Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Galaxy Astrophysics,Astrophysics - Instrumentation and Methods for Ast,Astrophysics - Solar and Stellar Astrophysics,astro},
mendeley-tags = {astro},
month = {dec},
primaryClass = {astro-ph.IM},
title = {{LSST Science Book, Version 2.0}},
year = {2009}
}
@article{2016A&A...589A..81C,
archivePrefix = {arXiv},
arxivId = {astro-ph.IM/1602.02167},
author = {Cantale, N and Courbin, F and Tewes, M and Jablonka, P and Meylan, G},
doi = {10.1051/0004-6361/201424003},
eprint = {1602.02167},
journal = {$\backslash$aap},
keywords = {astro,methods: data analysis},
mendeley-tags = {astro},
month = {may},
pages = {A81},
primaryClass = {astro-ph.IM},
title = {{Firedec: a two-channel finite-resolution image deconvolution algorithm}},
volume = {589},
year = {2016}
}
@article{nyquist1928certain,
author = {Nyquist, Harry},
journal = {Transactions of the American Institute of Electrical Engineers},
keywords = {astro},
mendeley-tags = {astro},
publisher = {publisher not identified},
title = {{Certain topics in telegraph transmission theory}},
year = {1928}
}
@phdthesis{1999PhDT........18C,
author = {Courbin, F},
keywords = {astro},
mendeley-tags = {astro},
school = {Institut d'astrophysique, Universite de Liege, Belgium; Observatoire de Paris Meudon - DAEC, FRance},
title = {{Deconvolution et combinaison optimale d'images astronomiques: application au cas des mirages gravitationnels}},
year = {1999}
}
@article{2011arXiv1110.3193L,
archivePrefix = {arXiv},
arxivId = {astro-ph.CO/1110.3193},
author = {Laureijs, R and Amiaux, J and Arduini, S and Augu{\`{e}}res, J.{\~{}}-. and Brinchmann, J and Cole, R and Cropper, M and Dabin, C and Duvet, L and Ealet, A and Et al.},
eprint = {1110.3193},
journal = {ArXiv e-prints},
keywords = {Astrophysics - Cosmology and Extragalactic Astroph,Astrophysics - Galaxy Astrophysics,astro},
mendeley-tags = {astro},
month = {oct},
primaryClass = {astro-ph.CO},
title = {{Euclid Definition Study Report}},
year = {2011}
}
@article{2008MmSAI..79.1251L,
author = {Letawe, Y and Magain, P and Letawe, G and Courbin, F and Hutsem{\'{e}}kers, D},
journal = {$\backslash$memsai},
keywords = {Quasars: host galaxies,astro,deconvolution,individuals: HE0354-5500},
mendeley-tags = {astro},
pages = {1251},
title = {{Study of the QSO HE0354-5500 with combined HST imaging and VLT spectroscopy . An example of a deconvolution-based method for probing the QSOs host galaxies characteristics}},
volume = {79},
year = {2008}
}
@article{Xu2014,
abstract = {Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.02776v1},
author = {Xu, Li and Ren, Jimmy SJ. and Liu, Ce and Jia, Jiaya},
eprint = {arXiv:1611.02776v1},
issn = {10495258},
journal = {Advances in neural information processing systems},
keywords = {astro},
mendeley-tags = {astro},
pages = {1--9},
title = {{Deep Convolutional Neural Network for Image Deconvolution}},
year = {2014}
}
@article{Schawinski2017,
author = {Schawinski, Kevin and Zhang, Ce and Zhang, Hantian and Fowler, Lucas and Santhanam, Gokula Krishnan},
doi = {10.1093/mnrasl/slx008},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Schawinski et al. - 2017 - Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution.pdf:pdf},
journal = {Monthly Notices of the Royal Astronomical Society: Letters},
keywords = {us},
mendeley-tags = {us},
month = {jan},
number = {1},
pages = {slx008},
publisher = {Oxford University Press},
title = {{Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit}},
url = {https://academic.oup.com/mnrasl/article-lookup/doi/10.1093/mnrasl/slx008},
volume = {120},
year = {2017}
}
@article{Meng2016,
abstract = {Vol. 5 onward called also Print-archive edition and represents articles already published online. Issues for 2005- published by Microtome publishing.},
author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
journal = {The Journal of Machine Learning Research},
keywords = {system},
mendeley-tags = {system},
number = {1},
pages = {1235--1241},
publisher = {MIT Press},
title = {{MLlib: machine learning in apache spark}},
url = {http://dl.acm.org/citation.cfm?id=2946679},
volume = {17},
year = {2016}
}
@article{Boehm2016,
author = {Boehm, Matthias and Surve, Arvind C. and Tatikonda, Shirish and Dusenberry, Michael W. and Eriksson, Deron and Evfimievski, Alexandre V. and Manshadi, Faraz Makari and Pansare, Niketan and Reinwald, Berthold and Reiss, Frederick R. and Sen, Prithviraj},
doi = {10.14778/3007263.3007279},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Boehm et al. - 2016 - SystemML.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
keywords = {system},
mendeley-tags = {system},
month = {sep},
number = {13},
pages = {1425--1436},
publisher = {VLDB Endowment},
title = {{SystemML}},
url = {http://dl.acm.org/citation.cfm?doid=3007263.3007279},
volume = {9},
year = {2016}
}
@article{Zaharia2016,
author = {Zaharia, Matei and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram},
doi = {10.1145/2934664},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {system},
mendeley-tags = {system},
month = {oct},
number = {11},
pages = {56--65},
title = {{Apache Spark}},
url = {http://dl.acm.org/citation.cfm?doid=3013530.2934664},
volume = {59},
year = {2016}
}
@inproceedings{Hadjis:2015:DANAC,
author = {Hadjis, Stefan and Abuzaid, Firas and Zhang, Ce and R{\'{e}}, Christopher},
booktitle = {DanaC},
title = {{{\{}Caffe Con Troll{\}}: Shallow Ideas to Speed Up Deep Learning}},
year = {2015}
}
@misc{TensorFlow:2015:WhitePaper,
annote = {Software available from tensorflow.org},
author = {Mart$\backslash$'$\backslash$in{\~{}}Abadi and Others},
title = {{{\{}TensorFlow{\}}: Large-Scale Machine Learning on Heterogeneous Systems}},
url = {http://tensorflow.org/},
year = {2015}
}
@inproceedings{Schleich:2016:SIGMOD,
author = {Schleich, Maximilian and Olteanu, Dan and Ciucanu, Radu},
booktitle = {SIGMOD},
title = {{Learning Linear Regression Models over Factorized Joins}},
year = {2016}
}
@article{Hellerstein:2012:VLDB,
author = {Hellerstein, Joseph M and R{\'{e}}, Christoper and Schoppmann, Florian and Wang, Daisy Zhe and Fratkin, Eugene and Gorajek, Aleksander and Ng, Kee Siong and Welton, Caleb and Feng, Xixuan and Li, Kun and Kumar, Arun},
journal = {Proc. VLDB Endow.},
title = {{The MADlib Analytics Library: Or MAD Skills, the SQL}},
year = {2012}
}
@article{Lin:2014:ICLR,
author = {Lin, M and Chen, Q and Yan, S},
journal = {ICLR},
title = {{Network In Network}},
year = {2014}
}
@inproceedings{Alvaro:2014:ICDE,
author = {Alvaro, Peter and Conway, Neil and Hellerstein, Joseph M and Maier, David},
booktitle = {ICDE},
title = {{Blazes: Coordination analysis for distributed programs}},
year = {2014}
}
@inproceedings{Brown:2010:SIGMOD,
author = {Brown, Paul G},
booktitle = {SIGMOD},
title = {{Overview of sciDB: Large Scale Array Storage, Processing and Analysis}},
year = {2010}
}
@misc{Keras:2015:Github,
author = {Chollet, Fran{\c{c}}ois},
howpublished = {$\backslash$url{\{}https://github.com/fchollet/keras{\}}},
publisher = {GitHub},
title = {{Keras}},
year = {2015}
}
@inproceedings{Kumar:2015:SIGMOD,
author = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M},
booktitle = {SIGMOD},
title = {{Learning Generalized Linear Models Over Normalized Data}},
year = {2015}
}
@book{MacGregor:2013:Book,
author = {MacGregor, John},
isbn = {1592299156, 9781592299157},
publisher = {SAP PRESS},
title = {{Predictive Analysis with SAP: The Comprehensive Guide}},
year = {2013}
}
@inproceedings{Seib:1991:PODS,
author = {Seib, J{\"{u}}rgen and Lausen, Georg},
booktitle = {PODS},
title = {{Parallelizing {\{}Datalog{\}} Programs by Generalized Pivoting}},
year = {1991}
}
@article{Cohen:2007:JACM,
author = {Cohen, Sara and Nutt, Werner and Sagiv, Yehoshua},
journal = {J. ACM},
title = {{Deciding Equivalences Among Conjunctive Aggregate Queries}},
year = {2007}
}
@article{Caffe:2014:ArXiv,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
journal = {arXiv preprint arXiv:1408.5093},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Low2012,
author = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
doi = {10.14778/2212351.2212354},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Low et al. - 2012 - Distributed GraphLab.pdf:pdf},
journal = {PVLDB},
keywords = {system},
mendeley-tags = {system},
month = {apr},
number = {8},
pages = {716--727},
publisher = {VLDB Endowment},
title = {{Distributed GraphLab}},
url = {http://dl.acm.org/citation.cfm?doid=2212351.2212354},
volume = {5},
year = {2012}
}
@article{Ragan-Kelley2013,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'{e}}do and Amarasinghe, Saman and Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'{e}}do and Amarasinghe, Saman},
doi = {10.1145/2499370.2462176},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing parallelism, locality, and recomputation in image processing.pdf:pdf},
isbn = {978-1-4503-2014-6},
journal = {ACM SIGPLAN Notices},
keywords = {dsl},
mendeley-tags = {dsl},
month = {jun},
number = {6},
pages = {519},
publisher = {ACM},
title = {{Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines}},
url = {http://dl.acm.org/citation.cfm?doid=2499370.2462176},
volume = {48},
year = {2013}
}
@techreport{Yu2014,
author = {Yu, Dong and Eversole, Adam and Seltzer, Mike and Yao, Kaisheng and Kuchaiev, Oleksii and Zhang, Yu and Seide, Frank and Huang, Zhiheng and Guenter, Brian and Wang, Huaming and Droppo, Jasha and Zweig, Geoffrey and Rossbach, Chris and Gao, Jie and Stolcke, Andreas and Currey, Jon and Slaney, Malcolm and Chen, Guoguo and Agarwal, Amit and Basoglu, Chris and Padmilac, Marko and Kamenev, Alexey and Ivanov, Vladimir and Cypher, Scott and Parthasarathi, Hari and Mitra, Bhaskar and Peng, Baolin and Huang, Xuedong},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2014 - An Introduction to Computational Networks and the Computational Network Toolkit(2).pdf:pdf},
institution = {Microsoft Research},
title = {{An Introduction to Computational Networks and the Computational Network Toolkit}},
url = {https://www.microsoft.com/en-us/research/publication/an-introduction-to-computational-networks-and-the-computational-network-toolkit/},
year = {2014}
}
@article{Truong2016,
author = {Truong, Leonard and Barik, Rajkishore and Totoni, Ehsan and Liu, Hai and Markley, Chick and Fox, Armando and Shpeisman, Tatiana and Truong, Leonard and Barik, Rajkishore and Totoni, Ehsan and Liu, Hai and Markley, Chick and Fox, Armando and Shpeisman, Tatiana},
doi = {10.1145/2980983.2908105},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Truong et al. - 2016 - Latte a language, compiler, and runtime for elegant and efficient deep neural networks(2).pdf:pdf},
isbn = {978-1-4503-4261-2},
journal = {ACM SIGPLAN Notices},
keywords = {dsl},
mendeley-tags = {dsl},
month = {jun},
number = {6},
pages = {209--223},
publisher = {ACM},
title = {{Latte: a language, compiler, and runtime for elegant and efficient deep neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=2980983.2908105},
volume = {51},
year = {2016}
}
@article{Moskewicz2016,
abstract = {In recent years, deep neural networks (DNNs), have yielded strong results on a wide range of applications. Graphics Processing Units (GPUs) have been one key enabling factor leading to the current popularity of DNNs. However, despite increasing hardware flexibility and software programming toolchain maturity, high efficiency GPU programming remains difficult: it suffers from high complexity, low productivity, and low portability. GPU vendors such as NVIDIA have spent enormous effort to write special-purpose DNN libraries. However, on other hardware targets, especially mobile GPUs, such vendor libraries are not generally available. Thus, the development of portable, open, high-performance, energy-efficient GPU code for DNN operations would enable broader deployment of DNN-based algorithms. Toward this end, this work presents a framework to enable productive, high-efficiency GPU programming for DNN computations across hardware platforms and programming models. In particular, the framework provides specific support for metaprogramming, autotuning, and DNN-tailored data types. Using our framework, we explore implementing DNN operations on three different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA GPUs, we show both portability between OpenCL and CUDA as well competitive performance compared to the vendor library. On Qualcomm GPUs, we show that our framework enables productive development of target-specific optimizations, and achieves reasonable absolute performance. Finally, On AMD GPUs, we show initial results that indicate our framework can yield reasonable performance on a new platform with minimal effort.},
archivePrefix = {arXiv},
arxivId = {1611.06945},
author = {Moskewicz, Matthew W. and Jannesari, Ali and Keutzer, Kurt},
eprint = {1611.06945},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Moskewicz, Jannesari, Keutzer - 2016 - A Metaprogramming and Autotuning Framework for Deploying Deep Learning Applications(2).pdf:pdf},
journal = {ArXiv e-prints},
keywords = {dsl},
mendeley-tags = {dsl},
month = {nov},
title = {{A Metaprogramming and Autotuning Framework for Deploying Deep Learning Applications}},
url = {http://arxiv.org/abs/1611.06945},
year = {2016}
}
@article{Gu1997,
author = {Gu, Hanzhong and Takahashi, Haruhisa},
doi = {10.1016/S0893-6080(97)00047-6},
journal = {Neural Networks},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {aug},
number = {6},
pages = {1089--1102},
title = {{Estimating Learning Curves of Concept Learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608097000476},
volume = {10},
year = {1997}
}
@article{Kabashima1992,
abstract = {For the problem of dividing the space originally partitioned by a blurred boundary, every learning algorithm can make the probability of incorrect prediction of an individual example $\epsilon$ decrease with the number of training examples t. We address here the question of how the asymptotic form of $\epsilon$(t) as well as its limit of convergence reflect the choice of learning algorithms. The error minimum algorithm is found to exhibit rather slow convergence of $\epsilon$(t) to its lower bound $\epsilon$0, $\epsilon$(t) - $\epsilon$0 ∼ O(t-2/3). Even for the purpose of minimizing prediction error, the maximum likelihood algorithm can be utilized as an alternative. If the true probability distribution happens to be contained in the family of hypothetical functions, then the boundary estimated from the hypothetical distribution function eventually converges to the best choice. Convergence of the prediction error is then $\epsilon$(t) - $\epsilon$0 ∼ O(t-1). If the true distribution is not available from the algorithm, however, the boundary generally does not converge to the...},
author = {Kabashima, Y. and Shinomoto, S.},
doi = {10.1162/neco.1992.4.5.712},
journal = {Neural Computation},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {sep},
number = {5},
pages = {712--719},
publisher = {MIT Press  238 Main St., Suite 500, Cambridge, MA 02142‐1046 USA journals-info@mit.edu},
title = {{Learning Curves for Error Minimum and Maximum Likelihood Algorithms}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.5.712},
volume = {4},
year = {1992}
}
@incollection{Gu2001,
author = {Gu, Baohua and Hu, Feifang and Liu, Huan},
booktitle = {WAIM},
doi = {10.1007/3-540-47714-4_29},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Gu, Hu, Liu - 2001 - Modelling Classification Performance for Large Data Sets(2).pdf:pdf},
keywords = {learning curve},
mendeley-tags = {learning curve},
pages = {317--328},
publisher = {Springer, Berlin, Heidelberg},
title = {{Modelling Classification Performance for Large Data Sets}},
url = {http://link.springer.com/10.1007/3-540-47714-4{\_}29},
year = {2001}
}
@article{Kang1993,
author = {Kang, Kukjin and Oh, Jong-Hoon and Kwon, Chulan and Park, Youngah},
doi = {10.1103/PhysRevE.48.4805},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Kang et al. - 1993 - Generalization in a two-layer neural network(2).pdf:pdf},
journal = {Physical Review E},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {dec},
number = {6},
pages = {4805--4809},
publisher = {American Physical Society},
title = {{Generalization in a two-layer neural network}},
url = {http://link.aps.org/doi/10.1103/PhysRevE.48.4805},
volume = {48},
year = {1993}
}
@article{Hamamoto1996,
author = {Hamamoto, Y. and Uchimura, S. and Tomita, S.},
doi = {10.1109/34.494648},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {may},
number = {5},
pages = {571--574},
title = {{On the behavior of artificial neural network classifiers in high-dimensional spaces}},
url = {http://ieeexplore.ieee.org/document/494648/},
volume = {18},
year = {1996}
}
@article{Amari1993a,
author = {Amari, Shun-ichi},
doi = {10.1016/0893-6080(93)90013-M},
journal = {Neural Networks},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {jan},
number = {2},
pages = {161--166},
title = {{A universal theorem on learning curves}},
url = {http://linkinghub.elsevier.com/retrieve/pii/089360809390013M},
volume = {6},
year = {1993}
}
@article{Amari1993,
abstract = {The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0 - m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of...},
author = {Amari, Shun-ichi and Murata, Noboru},
doi = {10.1162/neco.1993.5.1.140},
journal = {Neural Computation},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {jan},
number = {1},
pages = {140--153},
publisher = {MIT Press  238 Main St., Suite 500, Cambridge, MA 02142‐1046 USA journals-info@mit.edu},
title = {{Statistical Theory of Learning Curves under Entropic Loss Criterion}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1993.5.1.140},
volume = {5},
year = {1993}
}
@article{Watanabe2001,
abstract = {This article clarifies the relation between the learning curve and the algebraic geometrical structure of a nonidentifiable learning machine such as a multilayer neural network whose true parameter set is an analytic set with singular points. By using a concept in algebraic analysis, we rigorously prove that the Bayesian stochastic complexity or the free energy is asymptotically equal to $\lambda$1 logn − (m1 − 1) loglogn + constant, where n is the number of training samples and $\lambda$1 and m1 are the rational number and the natural number, which are determined as the birational invariant values of the singularities in the parameter space. Also we show an algorithm to calculate $\lambda$1 and m1 based on the resolution of singularities in algebraic geometry. In regular statistical models, 2$\lambda$1 is equal to the number of parameters and m1 = 1, whereas in nonregular models, such as multilayer networks, 2$\lambda$1 is not larger than the number of parameters and m1 ≥ 1. Since the increase of the stochastic complexity is equal to the learn...},
author = {Watanabe, Sumio},
doi = {10.1162/089976601300014402},
journal = {Neural Computation},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {apr},
number = {4},
pages = {899--933},
publisher = {MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Algebraic Analysis for Nonidentifiable Learning Machines}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976601300014402},
volume = {13},
year = {2001}
}
@inproceedings{DaikuiShourenHu,
author = {{Daikui Shouren Hu}},
booktitle = {[Proceedings] 1991 IEEE International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.1991.170357},
isbn = {0-7803-0227-3},
keywords = {dsl},
mendeley-tags = {dsl},
pages = {1606--1611},
publisher = {IEEE},
title = {{An object-oriented neural network language}},
url = {http://ieeexplore.ieee.org/document/170357/},
year = {1991}
}
@article{Korb1989,
author = {Korb, Thomas and Zell, Andreas},
doi = {10.1016/0165-6074(89)90043-4},
journal = {Microprocessing and Microprogramming},
keywords = {dsl},
mendeley-tags = {dsl},
month = {aug},
number = {1-5},
pages = {181--188},
title = {{A declarative neural network description language}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0165607489900434},
volume = {27},
year = {1989}
}
@article{Amari1992,
author = {Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
doi = {10.1162/neco.1992.4.4.605},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Amari, Fujita, Shinomoto - 1992 - Four Types of Learning Curves.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {jul},
number = {4},
pages = {605--618},
publisher = {MIT Press},
title = {{Four Types of Learning Curves}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.4.605},
volume = {4},
year = {1992}
}
@incollection{Yu2006,
address = {London},
author = {Yu, Weichuan and Wu, Baolin and Huang, Tao and Li, Xiaoye and Williams, Kenneth and Zhao, Hongyu},
booktitle = {Springer Handbook of Engineering Statistics},
doi = {10.1007/978-1-84628-288-1_34},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2006 - Statistical Methods in Proteomics(2).pdf:pdf},
keywords = {proteomics},
mendeley-tags = {proteomics},
pages = {623--638},
publisher = {Springer London},
title = {{Statistical Methods in Proteomics}},
url = {http://link.springer.com/10.1007/978-1-84628-288-1{\_}34},
year = {2006}
}
@article{Mukherjee2003,
abstract = {A statistical methodology for estimating dataset size requirements for classifying microarray data using learning curves is introduced. The goal is to use existing classification results to estimate dataset size requirements for future classification experiments and to evaluate the gain in accuracy and significance of classifiers built with additional data. The method is based on fitting inverse power-law models to construct empirical learning curves. It also includes a permutation test procedure to assess the statistical significance of classification performance for a given dataset size. This procedure is applied to several molecular classification problems representing a broad spectrum of levels of complexity.},
author = {Mukherjee, Sayan and Tamayo, Pablo and Rogers, Simon and Rifkin, Ryan and Engle, Anna and Campbell, Colin and Golub, Todd R. and Mesirov, Jill P.},
doi = {10.1089/106652703321825928},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {DNA MICROARRAYS,GENE EXPRESSION PROFILING,MICROARRAY ANALYSIS,MOLECULAR PATTERN RECOGNITION,SAMPLE SIZE ESTIMATION,learning curve},
mendeley-tags = {learning curve},
month = {apr},
number = {2},
pages = {119--142},
publisher = {Mary Ann Liebert, Inc.},
title = {{Estimating Dataset Size Requirements for Classifying DNA Microarray Data}},
url = {http://www.liebertonline.com/doi/abs/10.1089/106652703321825928},
volume = {10},
year = {2003}
}
@incollection{Smith2012,
address = {New York, NY},
author = {Smith, James Edward and Tahir, Muhammad Atif and Sannen, Davy and {Van Brussel}, Hendrik},
booktitle = {Learning in Non-Stationary Environments},
doi = {10.1007/978-1-4419-8020-5_6},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2012 - Making Early Predictions of the Accuracy of Machine Learning Classifiers(2).pdf:pdf},
keywords = {learning curve},
mendeley-tags = {learning curve},
pages = {125--151},
publisher = {Springer New York},
title = {{Making Early Predictions of the Accuracy of Machine Learning Classifiers}},
url = {http://link.springer.com/10.1007/978-1-4419-8020-5{\_}6},
year = {2012}
}
@article{Perlich2003,
author = {Perlich, Claudia and Provost, Foster and Simonoff, Jeffrey S.},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Perlich, Provost, Simonoff - 2003 - Tree Induction vs. Logistic Regression A Learning-Curve Analysis(2).pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {learning curve},
mendeley-tags = {learning curve},
number = {Jun},
pages = {211--255},
title = {{Tree Induction vs. Logistic Regression: A Learning-Curve Analysis}},
url = {http://www.jmlr.org/papers/v4/perlich03a.html},
volume = {4},
year = {2003}
}
@article{Haussler1996,
author = {Haussler, David and Kearns, Michael and Seung, H. Sebastian and Tishby, Naftali},
doi = {10.1023/A:1026499208981},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Haussler et al. - 1996 - Rigorous Learning Curve Bounds from Statistical Mechanics(2).pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {learning curve},
mendeley-tags = {learning curve},
number = {2/3},
pages = {195--236},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Rigorous Learning Curve Bounds from Statistical Mechanics}},
url = {http://link.springer.com/10.1023/A:1026499208981},
volume = {25},
year = {1996}
}
@misc{Kowalczyk1995,
author = {Kowalczyk, A. and Szymanski, J. and Bartlett, P. L. and Williamson, R. C.},
booktitle = {Proceedings of the 8th International Conference on Neural Information Processing Systems},
keywords = {learning curve},
mendeley-tags = {learning curve},
pages = {344--350},
publisher = {MIT Press},
title = {{Examples of learning curves from a modified VC-formalism}},
url = {http://dl.acm.org/citation.cfm?id=2998877},
year = {1995}
}
@article{Schuurmans1997,
author = {Schuurmans, Dale},
doi = {10.1006/jcss.1997.1505},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {learning curve},
mendeley-tags = {learning curve},
month = {aug},
number = {1},
pages = {140--160},
title = {{Characterizing Rational versus Exponential Learning Curves}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022000097915051},
volume = {55},
year = {1997}
}
@article{HanzhongGu2000,
author = {{Hanzhong Gu} and Takahashi, H.},
doi = {10.1109/34.879795},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {learning curve},
mendeley-tags = {learning curve},
number = {10},
pages = {1155--1167},
title = {{How bad may learning curves be?}},
url = {http://ieeexplore.ieee.org/document/879795/},
volume = {22},
year = {2000}
}
@misc{Cortes1993,
author = {Cortes, Corinna and Jackel, L. D. and Solla, Sara A. and Vapnik, Vladimir and Denker, John S.},
booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
keywords = {learning curve},
mendeley-tags = {learning curve},
pages = {327--334},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Learning curves: asymptotic values and rate of convergence}},
url = {http://dl.acm.org/citation.cfm?id=2987189.2987231},
year = {1993}
}
@article{Zhang2017,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {1611.03530},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Understanding deep learning requires rethinking generalization(2).pdf:pdf},
journal = {ICLR},
keywords = {learning curve},
mendeley-tags = {learning curve},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2017}
}
@article{Sparks2017,
abstract = {Modern advanced analytics applications make use of machine learning techniques and contain multiple steps of domain-specific and general-purpose processing with high resource requirements. We present KeystoneML, a system that captures and optimizes the end-to-end large-scale machine learning applications for high-throughput training in a distributed environment with a high-level API. This approach offers increased ease of use and higher performance over existing systems for large scale learning. We demonstrate the effectiveness of KeystoneML in achieving high quality statistical accuracy and scalable training using real world datasets in several domains. By optimizing execution KeystoneML achieves up to 15x training throughput over unoptimized execution on a real image classification application.},
archivePrefix = {arXiv},
arxivId = {1610.09451},
author = {Sparks, Evan R. and Venkataraman, Shivaram and Kaftan, Tomer and Franklin, Michael J. and Recht, Benjamin},
eprint = {1610.09451},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Sparks et al. - 2017 - KeystoneML Optimizing Pipelines for Large-Scale Advanced Analytics(2).pdf:pdf},
journal = {ICDE},
keywords = {system},
mendeley-tags = {system},
title = {{KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics}},
url = {http://arxiv.org/abs/1610.09451},
year = {2017}
}
@incollection{DeRaedt2012,
author = {{De Raedt}, Luc},
booktitle = {ECML},
doi = {10.1007/978-3-642-33460-3_2},
isbn = {978-3-642-33459-7},
keywords = {dsl},
mendeley-tags = {dsl},
pages = {2--3},
publisher = {Springer-Verlag},
title = {{Declarative Modeling for Machine Learning and Data Mining}},
url = {http://link.springer.com/10.1007/978-3-642-33460-3{\_}2},
year = {2012}
}
@inproceedings{Collobert_NIPSWORKSHOP_2011,
author = {Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Cl{\'{e}}ment},
booktitle = {BigLearn, NIPS Workshop},
keywords = {system},
mendeley-tags = {system},
title = {{Torch7: A Matlab-like Environment for Machine Learning}},
year = {2011}
}
@article{TheTheanoDevelopmentTeam2016,
abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
archivePrefix = {arXiv},
arxivId = {1605.02688},
author = {{The Theano Development Team}, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'{e}}d{\'{e}}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Br{\'{e}}bisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C{\^{o}}t{\'{e}}, Marc-Alexandre and C{\^{o}}t{\'{e}}, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M{\'{e}}lanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal{\'{a}}zs and Honari, Sina and Jain, Arjun and Jean, S{\'{e}}bastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C{\'{e}}sar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L{\'{e}}onard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merri{\"{e}}nboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran{\c{c}}ois and Schl{\"{u}}ter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, {\'{E}}tienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, J{\'{e}}r{\'{e}}mie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
eprint = {1605.02688},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/The Theano Development Team et al. - 2016 - Theano A Python framework for fast computation of mathematical expressions(2).pdf:pdf},
journal = {Arxiv},
keywords = {system},
mendeley-tags = {system},
month = {may},
title = {{Theano: A Python framework for fast computation of mathematical expressions}},
url = {http://arxiv.org/abs/1605.02688},
year = {2016}
}
@article{Chen2015,
abstract = {MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
archivePrefix = {arXiv},
arxivId = {1512.01274},
author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
eprint = {1512.01274},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2015 - MXNet A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems(3).pdf:pdf},
journal = {Arxiv},
keywords = {system},
mendeley-tags = {system},
month = {dec},
title = {{MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1512.01274},
year = {2015}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems(2).pdf:pdf},
journal = {ArXiv},
keywords = {system},
mendeley-tags = {system},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@misc{Cheng2016,
author = {Cheng, Gong and Zhou, Peicheng and Han, Junwei},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
pages = {2884--2893},
title = {{RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection}},
year = {2016}
}
@misc{Ngiam2010,
author = {Ngiam, Jiquan and Chen, Zhenghao and Chia, Daniel and Koh, Pang W. and Le, Quoc V. and Ng, Andrew Y.},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
pages = {1279--1287},
title = {{Tiled convolutional neural networks}},
year = {2010}
}
@article{Wu2015,
abstract = {This paper presents a new version of Dropout called Split Dropout (sDropout) and rotational convolution techniques to improve CNNs' performance on image classification. The widely used standard Dropout has advantage of preventing deep neural networks from overfitting by randomly dropping units during training. Our sDropout randomly splits the data into two subsets and keeps both rather than discards one subset. We also introduce two rotational convolution techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling convolution (FRPC) to boost CNNs' performance on the robustness for rotation transformation. These two techniques encode rotation invariance into the network without adding extra parameters. Experimental evaluations on ImageNet2012 classification task demonstrate that sDropout not only enhances the performance but also converges faster. Additionally, RPC and FRPC make CNNs more robust for rotation transformations. Overall, FRPC together with sDropout bring {\$}1.18\backslash{\%}{\$} (model of Zeiler and Fergus{\~{}}$\backslash$cite{\{}zeiler2013visualizing{\}}, 10-view, top-1) accuracy increase in ImageNet 2012 classification task compared to the original network.},
archivePrefix = {arXiv},
arxivId = {1507.08754},
author = {Wu, Fa and Hu, Peijun and Kong, Dexing},
eprint = {1507.08754},
journal = {arXiv:1507.08754 [cs]},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
title = {{Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification}},
url = {http://arxiv.org/abs/1507.08754{\%}5Cnhttp://www.arxiv.org/pdf/1507.08754.pdf},
year = {2015}
}
@article{Dieleman2016,
abstract = {Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.},
archivePrefix = {arXiv},
arxivId = {1602.02660},
author = {Dieleman, Sander and {De Fauw}, Jeffrey and Kavukcuoglu, Koray},
eprint = {1602.02660},
journal = {Arxiv},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
pages = {10},
title = {{Exploiting Cyclic Symmetry in Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1602.02660},
year = {2016}
}
@article{Marcos2016,
abstract = {We present a method for learning discriminative steerable filters using a shallow Convolutional Neural Network (CNN). We encode rotation invariance directly in the model by tying the weights of groups of filters to several rotated versions of the canonical filter in the group. These filters can be used to extract rotation invariant features well-suited for image classification. We test this learning procedure on a texture classification benchmark, where the orientations of the training images differ from those of the test images. We obtain results comparable to or better than the state-of-the-art. Besides numerical advantages, our proposed rotation invariant CNN decreases the number of parameters to be learned, thus showing more robustness in small training set scenarios than a standard CNN.},
archivePrefix = {arXiv},
arxivId = {1604.06720},
author = {Marcos, Diego and Volpi, Michele and Tuia, Devis},
eprint = {1604.06720},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
pages = {6},
title = {{Learning rotation invariant convolutional filters for texture classification}},
url = {http://arxiv.org/abs/1604.06720},
year = {2016}
}
@article{Cohen2016,
abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. By convolving over groups larger than the translation group, G-CNNs build representations that are equivariant to these groups, which makes it possible to greatly increase the degree of parameter sharing. We show how G-CNNs can be implemented with negligible computational overhead for discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. G-CNNs achieve state of the art results on rotated MNIST and significantly improve over a competitive baseline on augmented and non-augmented CIFAR-10.},
archivePrefix = {arXiv},
arxivId = {1602.07576},
author = {Cohen, Taco S. and Welling, Max},
eprint = {1602.07576},
journal = {Proceedings of The 33rd International Conference on Machine Learnin},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
pages = {2990--2999},
title = {{Group equivariant convolutional networks}},
url = {http://arxiv.org/abs/1602.07576},
year = {2016}
}
@article{Hart2016,
abstract = {The majority of galaxies in the local Universe exhibit spiral structure with a variety of forms. Many galaxies possess two prominent spiral arms, some have more, while others display a many-armed flocculent appearance. Spiral arms are associated with enhanced gas content and star-formation in the disks of low-redshift galaxies, so are important in the understanding of star-formation in the local universe. As both the visual appearance of spiral structure, and the mechanisms responsible for it vary from galaxy to galaxy, a reliable method for defining spiral samples with different visual morphologies is required. In this paper, we develop a new debiasing method to reliably correct for redshift-dependent bias in Galaxy Zoo 2, and release the new set of debiased classifications. Using these, a luminosity-limited sample of {\~{}}18,000 Sloan Digital Sky Survey spiral galaxies is defined, which are then further sub-categorised by spiral arm number. In order to explore how different spiral galaxies form, the demographics of spiral galaxies with different spiral arm numbers are compared. It is found that whilst all spiral galaxies occupy similar ranges of stellar mass and environment, many-armed galaxies display much bluer colours than their two-armed counterparts. We conclude that two-armed structure is ubiquitous in star-forming disks, whereas many-armed spiral structure appears to be a short-lived phase, associated with more recent, stochastic star-formation activity.},
author = {Hart, Ross E. and Bamford, Steven P. and Willett, Kyle W. and Masters, Karen L. and Cardamone, Carolin and Lintott, Chris J. and Mackay, Robert J. and Nichol, Robert C. and Rosslowe, Christopher K. and Simmons, Brooke D. and Smethurst, Rebecca J},
keywords = {1 i n t,data analysis,formation,galaxies,galaxy in the lo-,general,methods,most common type of,ral,ro d u c,rotation invariant,spi-,spiral galaxies are the,structure,t i o n},
mendeley-tags = {rotation invariant},
pages = {3663--3682},
title = {{Galaxy Zoo: comparing the demographics of spiral arm number and a new method for correcting redshift bias}},
url = {http://arxiv.org/abs/1607.01019},
volume = {3682},
year = {2016}
}
@article{Hoyle2016,
abstract = {We propose a new method to estimate the photometric redshift of galaxies by using the full galaxy image in each measured band. This method draws from the latest techniques and advances in machine learning, in particular Deep Neural Networks. We pass the entire multi-band galaxy image into the machine learning architecture to obtain a redshift estimate that is competitive, in terms of the measured point prediction metrics, with the best existing standard machine learning techniques. The standard techniques estimate redshifts using post-processed features, such as magnitudes and colours, which are extracted from the galaxy images and are deemed to be salient by the user. This new method removes the user from the photometric redshift estimation pipeline. However we do note that Deep Neural Networks require many orders of magnitude more computing resources than standard machine learning architectures, and as such are only tractable for making predictions on datasets of size ???50k before implementing parallelisation techniques.},
author = {Hoyle, B.},
journal = {Astronomy and Computing},
keywords = {Astronomy,Cosmology,Machine learning,rotation invariant},
mendeley-tags = {rotation invariant},
month = {jul},
pages = {34--40},
publisher = {Elsevier},
title = {{Measuring photometric redshifts using galaxy images and Deep Neural Networks}},
volume = {16},
year = {2016}
}
@inproceedings{Hossain2016,
author = {Hossain, Ibrahim and Khosravi, Abbas and Nahavandhi, Saeid},
booktitle = {2016 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2016.7727726},
isbn = {978-1-5090-0620-5},
keywords = {rotation invariant},
mendeley-tags = {rotation invariant},
month = {jul},
pages = {4048--4055},
publisher = {IEEE},
title = {{Active transfer learning and selective instance transfer with active learning for motor imagery based BCI}},
url = {http://ieeexplore.ieee.org/document/7727726/},
year = {2016}
}
@inproceedings{Rasti2013,
abstract = {In this paper, we propose a new super resolution technique based on the interpolation followed by registering them using iterative back projection (IBP). Low resolution images are being interpolated and then the interpolated images are being registered in order to generate a sharper high resolution image. The proposed technique has been tested on Lena, Elaine, Pepper, and Baboon. The quantitative peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) results as well as the visual results show the superiority of the proposed technique over the conventional and state-of-art image super resolution techniques. For Lena's image, the PSNR is 6.52 dB higher than the bicubic interpolation.},
author = {Rasti, Pejman and Demirel, Hasan and Anbarjafari, Gholamreza},
booktitle = {2013 21st Signal Processing and Communications Applications Conference, SIU 2013},
keywords = {Image registeration,Iterative back projection,Super resolution,rotation invariant},
mendeley-tags = {rotation invariant},
title = {{Image resolution enhancement by using interpolation followed by iterative back projection}},
year = {2013}
}
@article{Dieleman2015,
archivePrefix = {arXiv},
arxivId = {1503.07077},
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/MNRAS/STV632},
eprint = {1503.07077},
file = {:Users/cezhan/Library/Application Support/Mendeley Desktop/Downloaded/Dieleman, Willett, Dambre - 2015 - Rotation-invariant convolutional neural networks for galaxy morphology prediction.pdf:pdf},
keywords = {astro,rotation invariant},
mendeley-tags = {astro,rotation invariant},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
year = {2015}
}
